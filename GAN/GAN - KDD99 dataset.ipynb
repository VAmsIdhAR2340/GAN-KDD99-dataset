{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing for KDD99 Dataset**"
      ],
      "metadata": {
        "id": "L4jiBa2sMp86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "#LabelEncoder -> Encodes categorical features as numbers.\n",
        "#MinmaxScaler -> Scales features to range [0,1](default values).\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#TensorDataset -> Dataset wrapper which combines multiple tensors. Useful when you have features and labels in separate tensors and want to use them together\n",
        "#DataLoader -> Iterator provides many features like batching, shuffling, parallel data loading, etc\n",
        "\n",
        "class kddData(object):\n",
        "  def __init__(self,batch_size):\n",
        "    kddcup99 = sklearn.datasets.fetch_kddcup99()\n",
        "    self._encoder = {       #Private variable used within class\n",
        "          'protocal' : LabelEncoder(),\n",
        "          'service'  : LabelEncoder(),\n",
        "          'flag' : LabelEncoder(),\n",
        "          'label' : LabelEncoder()\n",
        "    } #self._encoder: Dictionary of LabelEncoder instances for categorical features\n",
        "\n",
        "    self._scaler = MinMaxScaler()\n",
        "    self.batch_size = batch_size\n",
        "    data_x, data_y = self._encode_and_scale_data(kddcup99.data,kddcup99.target)\n",
        "\n",
        "    self.train_dataset,self.test_dataset = self.__split_data_to_tensor(data_x, data_y)\n",
        "\n",
        "    self.train_dataloader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
        "    self.test_dataloader = DataLoader(self.test_dataset, self.batch_size, shuffle=True)\n",
        "\n",
        "  def _encode_and_scale_data(self,data_X,data_y):\n",
        "    # Analyzing the total input samples\n",
        "    print(\"output by Vamsidhar Vuddagiri\")\n",
        "    print(\"Original data samples: \", data_X[:5])\n",
        "    print(\"Original labels samples: \", data_y[:5])\n",
        "\n",
        "    #Normalizing the values that is converting to Labelencoder\n",
        "    #.fit -> Also known as training the encoder in this context. During the fitting process, the encoder examines all unique values in a categorical feature. assigns a unique integer to each unique category.\n",
        "    # Eg: Taking the feature protocol, it has many values eg tcp, udp, so on. Converting it into set to get unique values and converting to list to get uniques label encoders.\n",
        "    self._encoder['protocal'].fit(list(set(data_X[:, 1])))\n",
        "    self._encoder['service'].fit(list(set(data_X[:,2])))\n",
        "    self._encoder['flag'].fit(list(set(data_X[:,3])))\n",
        "\n",
        "    #Uses the fitted label encoders to transform categorical features into numerical values.\n",
        "    data_X[:,1] = self._encoder['protocal'].transform(data_X[:,1])\n",
        "    data_X[:,2] = self._encoder['service'].transform(data_X[:,2])\n",
        "    data_X[:,3] = self._encoder['flag'].transform(data_X[:,3])\n",
        "\n",
        "    #Normalizing the samples between [0,1]\n",
        "    data_X = self._scaler.fit_transform(data_X)\n",
        "    print(\"Normalized samples: \", data_X[:5])\n",
        "\n",
        "    #np.pad -> Extends array along with specified diensions. Constant specifies 0\n",
        "    data_X = np.pad(data_X, ((0, 0), (0, 64 - len(data_X[0]))), 'constant')\n",
        "    print(\"padded data samples：\", data_X[:5])\n",
        "\n",
        "    #Converts output labels to 0 & 1. 0 -> normal, 1 -> abnormal(anomaly)\n",
        "    data_y = np.where(data_y == b'normal.', 0, 1)\n",
        "    print(\"encoded data samples：\", data_y[:5])\n",
        "\n",
        "    return data_X , data_y\n",
        "\n",
        "  def __split_data_to_tensor(self, data_X, data_y):\n",
        "    normal_label = 0\n",
        "    normal_idx = data_y == normal_label\n",
        "    anomaly_idx = ~normal_idx\n",
        "\n",
        "    X_normal, X_anomaly = data_X[normal_idx], data_X[anomaly_idx]\n",
        "    y_normal, y_anomaly = data_y[normal_idx], data_y[anomaly_idx]\n",
        "\n",
        "    print(\"normal data samples：\", X_normal[:5])\n",
        "    print(\"anomaly data samples：\", X_anomaly[:5])\n",
        "\n",
        "    X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(X_normal, y_normal, test_size=0.3)\n",
        "    X_train_anomaly, X_test_anomaly, y_train_anomaly, y_test_anomaly = train_test_split(X_anomaly, y_anomaly, test_size=0.3)\n",
        "\n",
        "    X_train = np.concatenate((X_train_normal, X_train_anomaly), axis=0)\n",
        "    y_train = np.concatenate((y_train_normal, y_train_anomaly), axis=0)\n",
        "    X_test = np.concatenate((X_test_normal, X_test_anomaly), axis=0)\n",
        "    y_test = np.concatenate((y_test_normal, y_test_anomaly), axis=0)\n",
        "\n",
        "    print(\"training dataset samples：\", X_train[:5])\n",
        "    print(\"test dataset samples：\", X_test[:5])\n",
        "\n",
        "    #TensorDataset -> function in PyTorch is a dataset wrapper that combines multiple tensors into a single dataset.\n",
        "    train_dataset = TensorDataset(\n",
        "          torch.from_numpy(X_train.astype(np.float32)),\n",
        "          torch.from_numpy(y_train.astype(int))\n",
        "      )\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.from_numpy(X_test.astype(np.float32)),\n",
        "        torch.from_numpy(y_test.astype(int))\n",
        "    )\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4JtZUIAd0sVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "z_dim = 41    #Number of features in the dataset\n",
        "epochs = 50"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5vJpfCfWy2Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self,z_dim):  #initialization\n",
        "    super(Generator,self).__init__()   #Ensures that nn.module is properly initialized\n",
        "    self.model = nn.Sequential(       #nn.sequential -> Allows us to build neural network by stacking layers in the order they should execute\n",
        "        nn.Linear(z_dim,256),\n",
        "        nn.ReLU(True),            #Activation function to introduce non-linearity,True - indicates that operation should be done inplace, modifying input data without allocating new memory\n",
        "        nn.Linear(256,512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512,64),\n",
        "        #nn.Relu(True),\n",
        "        #nn.Linear(1024,28*28)\n",
        "        nn.Tanh()      #Activation function applies hyperbolic tangent function element wise. To scale output to a range suitable for the data. Espcially if the normalized data is between -1 to 1\n",
        "    )\n",
        "  def forward(self,z):\n",
        "    return self.model(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,z_dim):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        #nn.Linear(28*28,1024),\n",
        "        #nn.LeakyReLU(0.2,inplace = True) # This adds slope for negative values, which helps to address the dying of neurons\n",
        "        #This small gradient prevents the weights from not updating, which helps avoid the situation where neurons stop learning (dying) and stay inactive.\n",
        "        # We are using this in GAN discriminator for many reasons Avoiding sparse gradients, Imporoved\n",
        "        #traing stability, discriminator sensitivity, architecture compatibility\n",
        "        nn.Linear(64,512),\n",
        "        nn.LeakyReLU(0.2,inplace=True),\n",
        "        nn.Linear(512,256),\n",
        "        nn.LeakyReLU(0.2,inplace=True),\n",
        "        nn.Linear(256,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "y48-7MwW6muJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task - 1:**"
      ],
      "metadata": {
        "id": "SclIVT8cSC4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss-D The discriminator's objective is to distinguish between real data (from the training set) and fake data (generated by the generator).\n",
        "# A lower Loss D indicates that the discriminator is getting better at distinguishing real from fake data.\n",
        "#loss - G The generator's objective is to produce data that is realistic enough to fool the discriminator.\n",
        "# A lower Loss G suggests that the generator is producing data that is more likely to be classified as real by the discriminator.\n",
        "#GANs operate in a minimax game where the generator tries to minimize Loss G while the discriminator tries to minimize Loss D.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "kdd_data = kddData(batch_size=batch_size)\n",
        "train_dl = kdd_data.train_dataloader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step -1 Train the GAN on normal data only, so the generator learns to produce realistic normal data.\n",
        "\n",
        "generator = Generator(z_dim).to(device)\n",
        "discriminator = Discriminator(z_dim).to(device)\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "epochs = 2\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        real_data, labels = data\n",
        "        normal_data = real_data[labels == 0]  # Extract only normal data based on labels\n",
        "\n",
        "        # Check if there's any normal data in the batch\n",
        "        if normal_data.size(0) == 0:\n",
        "            continue  # Skip batch if there's no normal data\n",
        "\n",
        "        normal_data = normal_data.to(device)\n",
        "        batch_size = normal_data.size(0)\n",
        "        valid = torch.ones(batch_size, 1).to(device)\n",
        "        fake = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "\n",
        "        gen_data = generator(z)\n",
        "        g_loss = criterion(discriminator(gen_data), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # The discriminator is trained to classify normal_data as real by comparing it to valid\n",
        "        real_loss = criterion(discriminator(normal_data), valid)\n",
        "        fake_loss = criterion(discriminator(gen_data.detach()), fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Output by Vamsidhar Vuddagiri: Epoch [{epoch+1}/{epochs}] Batch [{i}/{len(train_dl)}] \\\n",
        "                  Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtcMUrpDUSyW",
        "outputId": "a9599948-5d60-42e5-dc70-7bcbee2c5968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output by Vamsidhar Vuddagiri\n",
            "Original data samples:  [[0 b'tcp' b'http' b'SF' 181 5450 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 8 0.0\n",
            "  0.0 0.0 0.0 1.0 0.0 0.0 9 9 1.0 0.0 0.11 0.0 0.0 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' b'SF' 239 486 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 8 0.0\n",
            "  0.0 0.0 0.0 1.0 0.0 0.0 19 19 1.0 0.0 0.05 0.0 0.0 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' b'SF' 235 1337 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 8 0.0\n",
            "  0.0 0.0 0.0 1.0 0.0 0.0 29 29 1.0 0.0 0.03 0.0 0.0 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' b'SF' 219 1337 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 6 6 0.0\n",
            "  0.0 0.0 0.0 1.0 0.0 0.0 39 39 1.0 0.0 0.03 0.0 0.0 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' b'SF' 217 2032 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 6 6 0.0\n",
            "  0.0 0.0 0.0 1.0 0.0 0.0 49 49 1.0 0.0 0.02 0.0 0.0 0.0 0.0 0.0]]\n",
            "Original labels samples:  [b'normal.' b'normal.' b'normal.' b'normal.' b'normal.']\n",
            "Normalized samples:  [[0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  2.61041764e-07 1.05713002e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.52941176e-02\n",
            "  3.52941176e-02 1.00000000e+00 0.00000000e+00 1.10000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.44690506e-07 9.42688423e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 7.45098039e-02\n",
            "  7.45098039e-02 1.00000000e+00 0.00000000e+00 5.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.38921627e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.13725490e-01\n",
            "  1.13725490e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.15846112e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.52941176e-01\n",
            "  1.52941176e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.12961673e-07 3.94144625e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.92156863e-01\n",
            "  1.92156863e-01 1.00000000e+00 0.00000000e+00 2.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]]\n",
            "padded data samples： [[0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  2.61041764e-07 1.05713002e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.52941176e-02\n",
            "  3.52941176e-02 1.00000000e+00 0.00000000e+00 1.10000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.44690506e-07 9.42688423e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 7.45098039e-02\n",
            "  7.45098039e-02 1.00000000e+00 0.00000000e+00 5.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.38921627e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.13725490e-01\n",
            "  1.13725490e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.15846112e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.52941176e-01\n",
            "  1.52941176e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.12961673e-07 3.94144625e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.92156863e-01\n",
            "  1.92156863e-01 1.00000000e+00 0.00000000e+00 2.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "encoded data samples： [0 0 0 0 0]\n",
            "normal data samples： [[0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  2.61041764e-07 1.05713002e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.52941176e-02\n",
            "  3.52941176e-02 1.00000000e+00 0.00000000e+00 1.10000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.44690506e-07 9.42688423e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 7.45098039e-02\n",
            "  7.45098039e-02 1.00000000e+00 0.00000000e+00 5.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.38921627e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.56555773e-02 1.56555773e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.13725490e-01\n",
            "  1.13725490e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.15846112e-07 2.59336301e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.52941176e-01\n",
            "  1.52941176e-01 1.00000000e+00 0.00000000e+00 3.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.12961673e-07 3.94144625e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 1.17416830e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.92156863e-01\n",
            "  1.92156863e-01 1.00000000e+00 0.00000000e+00 2.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "anomaly data samples： [[3.15452005e-03 5.00000000e-01 8.61538462e-01 9.00000000e-01\n",
            "  2.17919395e-06 5.73565775e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e-01 0.00000000e+00 1.00000000e+00\n",
            "  2.26244344e-03 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.57142857e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.92156863e-03\n",
            "  1.17647059e-02 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  6.70000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.22895987e-03 5.00000000e-01 8.61538462e-01 9.00000000e-01\n",
            "  2.50225116e-06 5.36517732e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e-01 0.00000000e+00 1.00000000e+00\n",
            "  2.26244344e-03 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.57142857e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 7.84313725e-03\n",
            "  1.56862745e-02 1.00000000e+00 0.00000000e+00 5.00000000e-01\n",
            "  5.00000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.35438633e-03 5.00000000e-01 8.61538462e-01 9.00000000e-01\n",
            "  4.05263733e-07 2.52353424e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 6.66666667e-02 0.00000000e+00 1.00000000e+00\n",
            "  1.13122172e-03 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.42857143e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.92156863e-03\n",
            "  3.92156863e-02 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  3.00000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.28603268e-04 5.00000000e-01 8.61538462e-01 9.00000000e-01\n",
            "  3.87957096e-07 4.52529237e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 2.01409869e-03\n",
            "  7.14285714e-02 5.00000000e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 2.70588235e-01\n",
            "  7.84313725e-03 3.00000000e-02 6.00000000e-02 1.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 8.61538462e-01 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.17416830e-02 9.78473581e-03\n",
            "  8.30000000e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  8.30000000e-01 3.30000000e-01 0.00000000e+00 1.96078431e-02\n",
            "  2.35294118e-02 1.00000000e+00 0.00000000e+00 2.00000000e-01\n",
            "  3.30000000e-01 1.00000000e+00 8.30000000e-01 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "training dataset samples： [[1.98700475e-02 1.00000000e+00 6.15384615e-01 9.00000000e-01\n",
            "  2.09121855e-07 2.03667252e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  3.92156863e-03 0.00000000e+00 5.60000000e-01 9.60000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.01423915e-07 2.54293112e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 2.15264188e-02 2.15264188e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 4.31372549e-02\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 9.00000000e-02\n",
            "  4.00000000e-02 0.00000000e+00 1.00000000e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  4.08148172e-07 1.39715735e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.36986301e-02 1.36986301e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 7.69230769e-01 9.00000000e-01\n",
            "  9.86189823e-06 7.42900548e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 3.91389432e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  8.62745098e-02 9.00000000e-02 7.00000000e-02 0.00000000e+00\n",
            "  0.00000000e+00 8.40000000e-01 0.00000000e+00 1.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.02864784e-04 5.00000000e-01 7.69230769e-01 9.00000000e-01\n",
            "  2.93924373e-06 6.53674894e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 1.95694716e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.37254902e-01\n",
            "  5.60784314e-01 4.20000000e-01 5.00000000e-02 1.00000000e-02\n",
            "  1.00000000e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "test dataset samples： [[3.42882614e-05 5.00000000e-01 7.69230769e-01 9.00000000e-01\n",
            "  1.65855264e-06 6.40097078e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 5.87084149e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 6.70000000e-01 9.09803922e-01\n",
            "  5.09803922e-01 5.50000000e-01 2.00000000e-02 0.00000000e+00\n",
            "  2.00000000e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.27383869e-07 8.70919963e-05 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 5.08806262e-02 5.08806262e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 1.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.95694716e-03 3.91389432e-03\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 7.45098039e-02\n",
            "  3.52941176e-02 4.70000000e-01 1.60000000e-01 5.00000000e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.70000000e-01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  3.63439362e-07 8.49389425e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 2.54403131e-02 2.93542074e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.30000000e-01 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 5.00000000e-01 3.38461538e-01 9.00000000e-01\n",
            "  2.94212817e-07 3.43130827e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.02348337e-02 8.02348337e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 3.72549020e-01\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e-02\n",
            "  2.00000000e-02 0.00000000e+00 0.00000000e+00 5.70000000e-01\n",
            "  2.90000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [0/5404]                   Loss D: 0.6950, Loss G: 0.6751\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [100/5404]                   Loss D: 0.5308, Loss G: 0.9463\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [200/5404]                   Loss D: 0.1417, Loss G: 2.7512\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [300/5404]                   Loss D: 0.0443, Loss G: 3.9541\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [400/5404]                   Loss D: 0.2311, Loss G: 1.9286\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [500/5404]                   Loss D: 0.4312, Loss G: 0.8919\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [600/5404]                   Loss D: 0.5883, Loss G: 0.7569\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [700/5404]                   Loss D: 0.9627, Loss G: 0.5059\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [800/5404]                   Loss D: 0.7616, Loss G: 0.6941\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [900/5404]                   Loss D: 0.5935, Loss G: 0.7907\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1000/5404]                   Loss D: 0.5694, Loss G: 0.8771\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1100/5404]                   Loss D: 0.4127, Loss G: 1.0491\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1200/5404]                   Loss D: 0.4977, Loss G: 1.0488\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1300/5404]                   Loss D: 0.4859, Loss G: 1.0150\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1400/5404]                   Loss D: 0.6869, Loss G: 0.8799\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1500/5404]                   Loss D: 0.3467, Loss G: 1.1284\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1600/5404]                   Loss D: 0.3894, Loss G: 1.1970\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1700/5404]                   Loss D: 0.2466, Loss G: 1.5822\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1800/5404]                   Loss D: 0.1762, Loss G: 1.7536\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [1900/5404]                   Loss D: 0.3489, Loss G: 1.3182\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2000/5404]                   Loss D: 0.2180, Loss G: 1.7410\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2100/5404]                   Loss D: 0.1961, Loss G: 2.4547\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2200/5404]                   Loss D: 0.1872, Loss G: 1.6732\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2300/5404]                   Loss D: 0.3066, Loss G: 1.5220\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2400/5404]                   Loss D: 0.1623, Loss G: 2.1481\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2500/5404]                   Loss D: 0.1485, Loss G: 2.0273\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2600/5404]                   Loss D: 0.1325, Loss G: 2.0313\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2700/5404]                   Loss D: 0.1695, Loss G: 1.7428\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2800/5404]                   Loss D: 0.1727, Loss G: 1.8017\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [2900/5404]                   Loss D: 0.1500, Loss G: 2.0919\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3000/5404]                   Loss D: 0.3063, Loss G: 2.0466\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3100/5404]                   Loss D: 0.0341, Loss G: 3.0816\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3200/5404]                   Loss D: 0.0977, Loss G: 2.4085\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3300/5404]                   Loss D: 0.1342, Loss G: 1.8063\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3400/5404]                   Loss D: 0.2156, Loss G: 1.9842\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3500/5404]                   Loss D: 0.3176, Loss G: 1.7361\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3600/5404]                   Loss D: 0.1394, Loss G: 2.2437\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3700/5404]                   Loss D: 0.0997, Loss G: 2.4833\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3800/5404]                   Loss D: 0.1732, Loss G: 2.1052\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [3900/5404]                   Loss D: 0.2956, Loss G: 2.1391\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4000/5404]                   Loss D: 0.1511, Loss G: 1.8360\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4100/5404]                   Loss D: 0.1625, Loss G: 1.9270\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4200/5404]                   Loss D: 0.2034, Loss G: 1.5175\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4300/5404]                   Loss D: 0.1484, Loss G: 1.8778\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4400/5404]                   Loss D: 0.2085, Loss G: 1.6608\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4500/5404]                   Loss D: 0.3081, Loss G: 2.1503\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4600/5404]                   Loss D: 0.3885, Loss G: 2.4436\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4700/5404]                   Loss D: 0.2463, Loss G: 2.5352\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4800/5404]                   Loss D: 0.2486, Loss G: 2.5105\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [4900/5404]                   Loss D: 0.2269, Loss G: 2.0775\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [5000/5404]                   Loss D: 0.1434, Loss G: 1.8913\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [5100/5404]                   Loss D: 0.1466, Loss G: 1.8192\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [5200/5404]                   Loss D: 0.3864, Loss G: 0.9496\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [5300/5404]                   Loss D: 0.2375, Loss G: 1.9619\n",
            "Output by Vamsidhar Vuddagiri: Epoch [1/2] Batch [5400/5404]                   Loss D: 0.1621, Loss G: 1.7927\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [0/5404]                   Loss D: 0.1131, Loss G: 2.0881\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [100/5404]                   Loss D: 0.1311, Loss G: 1.9330\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [200/5404]                   Loss D: 0.2342, Loss G: 2.2422\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [300/5404]                   Loss D: 0.4258, Loss G: 0.9786\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [400/5404]                   Loss D: 0.1635, Loss G: 2.1159\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [500/5404]                   Loss D: 0.1349, Loss G: 2.4240\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [600/5404]                   Loss D: 0.1953, Loss G: 1.9852\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [700/5404]                   Loss D: 0.2342, Loss G: 2.8073\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [800/5404]                   Loss D: 0.2429, Loss G: 1.4780\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [900/5404]                   Loss D: 0.2495, Loss G: 1.1383\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1000/5404]                   Loss D: 0.1201, Loss G: 1.9374\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1100/5404]                   Loss D: 0.1649, Loss G: 2.7714\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1200/5404]                   Loss D: 0.3150, Loss G: 1.6818\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1300/5404]                   Loss D: 0.1891, Loss G: 1.8916\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1400/5404]                   Loss D: 0.1802, Loss G: 2.0346\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1500/5404]                   Loss D: 0.2474, Loss G: 2.3670\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1600/5404]                   Loss D: 0.2184, Loss G: 1.4398\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1700/5404]                   Loss D: 0.3081, Loss G: 2.0532\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1800/5404]                   Loss D: 0.4383, Loss G: 2.2918\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [1900/5404]                   Loss D: 0.2531, Loss G: 1.5016\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2000/5404]                   Loss D: 0.1601, Loss G: 1.9043\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2100/5404]                   Loss D: 0.2837, Loss G: 1.1572\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2200/5404]                   Loss D: 0.2642, Loss G: 1.2619\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2300/5404]                   Loss D: 0.4524, Loss G: 1.9478\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2400/5404]                   Loss D: 0.3231, Loss G: 1.3089\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2500/5404]                   Loss D: 0.3256, Loss G: 1.5973\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2600/5404]                   Loss D: 0.5861, Loss G: 2.0093\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2700/5404]                   Loss D: 0.2428, Loss G: 1.2196\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2800/5404]                   Loss D: 0.1349, Loss G: 2.6166\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [2900/5404]                   Loss D: 0.2354, Loss G: 1.9651\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3000/5404]                   Loss D: 0.1933, Loss G: 2.1753\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3100/5404]                   Loss D: 0.1774, Loss G: 1.9699\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3200/5404]                   Loss D: 0.2186, Loss G: 1.8260\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3300/5404]                   Loss D: 0.1812, Loss G: 2.2467\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3400/5404]                   Loss D: 0.3562, Loss G: 2.0058\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3500/5404]                   Loss D: 0.2027, Loss G: 2.2329\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3600/5404]                   Loss D: 0.2001, Loss G: 2.1839\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3700/5404]                   Loss D: 0.2401, Loss G: 1.5849\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3800/5404]                   Loss D: 0.3533, Loss G: 1.7006\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [3900/5404]                   Loss D: 0.2849, Loss G: 1.9066\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4000/5404]                   Loss D: 0.2199, Loss G: 1.2853\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4100/5404]                   Loss D: 0.2379, Loss G: 1.9839\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4200/5404]                   Loss D: 0.2347, Loss G: 1.2338\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4300/5404]                   Loss D: 0.1934, Loss G: 1.4068\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4400/5404]                   Loss D: 0.3723, Loss G: 0.9388\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4500/5404]                   Loss D: 0.3912, Loss G: 1.4604\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4600/5404]                   Loss D: 0.2676, Loss G: 1.3193\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4700/5404]                   Loss D: 0.4967, Loss G: 1.0726\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4800/5404]                   Loss D: 0.1493, Loss G: 1.7581\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [4900/5404]                   Loss D: 0.5025, Loss G: 1.7345\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [5000/5404]                   Loss D: 0.5629, Loss G: 1.2397\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [5100/5404]                   Loss D: 0.1636, Loss G: 2.0739\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [5200/5404]                   Loss D: 0.2552, Loss G: 1.6685\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [5300/5404]                   Loss D: 0.3420, Loss G: 1.0827\n",
            "Output by Vamsidhar Vuddagiri: Epoch [2/2] Batch [5400/5404]                   Loss D: 0.2198, Loss G: 1.8488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step - 2 After training, use the Discriminator's Score: For a data instance in the test-dataset, pass it through the discriminator.\n",
        "\n",
        "discriminator.eval()\n",
        "with torch.no_grad():\n",
        "  discriminator_anomaly_score = []\n",
        "  for data in kdd_data.test_dataloader:\n",
        "    test_data,labels = data\n",
        "    test_data = test_data.to(device)\n",
        "\n",
        "    output = discriminator(test_data)\n",
        "    discriminator_anomaly_score.extend(output.cpu().numpy())\n",
        "\n",
        "for i in discriminator_anomaly_score[:10]:\n",
        "  print(i,end = ' ')\n",
        "print('\\n')\n",
        "print(\"Output By Vamsidhar Vuddagiri\")\n",
        "print(len(discriminator_anomaly_score))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKfE8Tp2jTKp",
        "outputId": "8ca3b105-fb5e-47e6-80fc-c7603edac68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04108508] [0.04108508] [0.02047094] [0.0194628] [0.97011864] [0.04108508] [0.96617436] [0.68150353] [0.06674822] [0.04108508] \n",
            "\n",
            "Output By Vamsidhar Vuddagiri\n",
            "148207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-2 After training a GAN with normal data, we utilize Generator to detect anomaly data:**"
      ],
      "metadata": {
        "id": "FJr7plG-kPoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step - 1 Train the GAN on normal data, is done in the above code cells\n",
        "\n",
        "generator.eval()\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "generator_recon_error = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for data in kdd_data.test_dataloader:\n",
        "    test_data,labels= data\n",
        "    test_data = test_data.to(device)\n",
        "    z = torch.randn(test_data.size(0),z_dim).to(device)\n",
        "    gen_data = generator(z) #Step - 2 Generating synthetic data\n",
        "\n",
        "    error = mse_loss(test_data,gen_data) #step - 3 Calculate Reconstruction Error\n",
        "    generator_recon_error.append(error.item())\n",
        "#item -> Using .item() in PyTorch is necessary when one want to extract a scalar value (like a loss or a single element from a tensor) from a PyTorch tensor.\n",
        "\n",
        "print(\"Output By Vamsidhar Vuddagiri\")\n",
        "print(generator_recon_error)\n",
        "print(generator_recon_error[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adtSbeMHkPQt",
        "outputId": "700e1858-f8d9-4d44-a057-d9b8301d01e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output By Vamsidhar Vuddagiri\n",
            "[0.08483822643756866, 0.07727351784706116, 0.08414161205291748, 0.08524221926927567, 0.08313155174255371, 0.08663501590490341, 0.08269824087619781, 0.08233568072319031, 0.08717755973339081, 0.08617262542247772, 0.07647980749607086, 0.08966077864170074, 0.08200601488351822, 0.08586807548999786, 0.08271805197000504, 0.07702021300792694, 0.084801584482193, 0.09080754965543747, 0.07904180884361267, 0.076869897544384, 0.09350565075874329, 0.07832103967666626, 0.07831224799156189, 0.07695070654153824, 0.08650258183479309, 0.08212855458259583, 0.08364462852478027, 0.08121958374977112, 0.07860980927944183, 0.08284422010183334, 0.07994355261325836, 0.07463324815034866, 0.08462869375944138, 0.0832321047782898, 0.08702519536018372, 0.08111811429262161, 0.0834161564707756, 0.07985278964042664, 0.07940258085727692, 0.08205792307853699, 0.08309320360422134, 0.07962082326412201, 0.08564326167106628, 0.08609050512313843, 0.07992836833000183, 0.08595743030309677, 0.08457237482070923, 0.08247089385986328, 0.08932511508464813, 0.08385114371776581, 0.08333785086870193, 0.08693863451480865, 0.08188880980014801, 0.08561812341213226, 0.08361902832984924, 0.08754390478134155, 0.08339649438858032, 0.08461201936006546, 0.07824098318815231, 0.08331277966499329, 0.08941435813903809, 0.0811692327260971, 0.08262374997138977, 0.08299773931503296, 0.0897509753704071, 0.08576034009456635, 0.07501085847616196, 0.08376599848270416, 0.07585465908050537, 0.08157883584499359, 0.08248533308506012, 0.07764245569705963, 0.09203417599201202, 0.07698781043291092, 0.07864148914813995, 0.07480254769325256, 0.0812206119298935, 0.08175041526556015, 0.0907018631696701, 0.08407983928918839, 0.08927787840366364, 0.08544149994850159, 0.08277682960033417, 0.08746730536222458, 0.08487004041671753, 0.07600495219230652, 0.08912496268749237, 0.08164234459400177, 0.07777388393878937, 0.07337456941604614, 0.08627094328403473, 0.07941119372844696, 0.08433091640472412, 0.08488011360168457, 0.08102399110794067, 0.0831127017736435, 0.07837104797363281, 0.0835457444190979, 0.08007153123617172, 0.08159011602401733, 0.07533387839794159, 0.08854439854621887, 0.0909387543797493, 0.08650043606758118, 0.08185446262359619, 0.07522503286600113, 0.08292464911937714, 0.09248349070549011, 0.08097968995571136, 0.07723931968212128, 0.07784099876880646, 0.07877938449382782, 0.08974023163318634, 0.08248697966337204, 0.08228083699941635, 0.08188357204198837, 0.08853142708539963, 0.08245918154716492, 0.0918198972940445, 0.08093169331550598, 0.08294893056154251, 0.07991601526737213, 0.08097071945667267, 0.08618178963661194, 0.08870336413383484, 0.07823100686073303, 0.08185078203678131, 0.08040381222963333, 0.09065498411655426, 0.08767230063676834, 0.08954785764217377, 0.07507257163524628, 0.09227462857961655, 0.08111971616744995, 0.08427555859088898, 0.07601544260978699, 0.07324804365634918, 0.07610255479812622, 0.08354414999485016, 0.0782897025346756, 0.07853792607784271, 0.08032983541488647, 0.09199100732803345, 0.07370295375585556, 0.0838417187333107, 0.08149959146976471, 0.0875144675374031, 0.08754031360149384, 0.07722359150648117, 0.07564213871955872, 0.07967614382505417, 0.08538433164358139, 0.07822433859109879, 0.08321527391672134, 0.08128590136766434, 0.08278852701187134, 0.07893325388431549, 0.0782628133893013, 0.08167116343975067, 0.0763264149427414, 0.07885557413101196, 0.07559148222208023, 0.08945260941982269, 0.08355250954627991, 0.08151908963918686, 0.07063935697078705, 0.08117851614952087, 0.08548514544963837, 0.08592646569013596, 0.08109541982412338, 0.07947616279125214, 0.07959909737110138, 0.08073767274618149, 0.08742006123065948, 0.08241382241249084, 0.07551179826259613, 0.08384798467159271, 0.07628390938043594, 0.07395820319652557, 0.08514188230037689, 0.08052539825439453, 0.08438791334629059, 0.07808860391378403, 0.07735814154148102, 0.08020061999559402, 0.07394175231456757, 0.0799703299999237, 0.08270065486431122, 0.080525241792202, 0.08120469748973846, 0.08803626894950867, 0.07962378114461899, 0.08253266662359238, 0.07997582852840424, 0.07456396520137787, 0.08865680545568466, 0.08675357699394226, 0.0772244855761528, 0.08557146787643433, 0.07496309280395508, 0.081953264772892, 0.08271708339452744, 0.08604905754327774, 0.07482331991195679, 0.07802240550518036, 0.08385391533374786, 0.08081837743520737, 0.08523720502853394, 0.0775706022977829, 0.08023633807897568, 0.07582301646471024, 0.07385651767253876, 0.08148255944252014, 0.08203798532485962, 0.08091142028570175, 0.08049324154853821, 0.08811218291521072, 0.07994265854358673, 0.08828103542327881, 0.08631107211112976, 0.07792959362268448, 0.08272643387317657, 0.0756991058588028, 0.08207210898399353, 0.09028297662734985, 0.07745273411273956, 0.07986529171466827, 0.08574575185775757, 0.08017664402723312, 0.07537428289651871, 0.09000704437494278, 0.07610777020454407, 0.08474614471197128, 0.08576390147209167, 0.08505607396364212, 0.08448377251625061, 0.07877170294523239, 0.07836809754371643, 0.07666145265102386, 0.08229823410511017, 0.0807955265045166, 0.07790784537792206, 0.08094646781682968, 0.08572541922330856, 0.08211158215999603, 0.08274365216493607, 0.0899224728345871, 0.07788928598165512, 0.08626771718263626, 0.08075742423534393, 0.08355888724327087, 0.08255234360694885, 0.08228956162929535, 0.07699349522590637, 0.08020752668380737, 0.0856066644191742, 0.09060166776180267, 0.08328741788864136, 0.07864709943532944, 0.07812157273292542, 0.07804252952337265, 0.0821223333477974, 0.0885142982006073, 0.0812777727842331, 0.08171124756336212, 0.07992812991142273, 0.08574190735816956, 0.08922195434570312, 0.08598251640796661, 0.08456568419933319, 0.08081021904945374, 0.0821070447564125, 0.07991409301757812, 0.0831701010465622, 0.0795433446764946, 0.08209553360939026, 0.08136548101902008, 0.08430083841085434, 0.08532017469406128, 0.08646509796380997, 0.0810052901506424, 0.0844859704375267, 0.07128605246543884, 0.08415219932794571, 0.07793748378753662, 0.08352689445018768, 0.08382777869701385, 0.08829766511917114, 0.07787450402975082, 0.0820159837603569, 0.07968339323997498, 0.08126720786094666, 0.07753419131040573, 0.07995457947254181, 0.08035184442996979, 0.08513589203357697, 0.08452238142490387, 0.08347620069980621, 0.08725497126579285, 0.08727969229221344, 0.08335469663143158, 0.08421484380960464, 0.07961997389793396, 0.08750635385513306, 0.08060581982135773, 0.07663008570671082, 0.08387301862239838, 0.08232057839632034, 0.08574158698320389, 0.08359097689390182, 0.07402099668979645, 0.07834391295909882, 0.08134965598583221, 0.0870189443230629, 0.0841863602399826, 0.07851575314998627, 0.08323326706886292, 0.07924173772335052, 0.08630284667015076, 0.087730273604393, 0.07915417104959488, 0.07652975618839264, 0.08534237742424011, 0.08743458986282349, 0.08524028956890106, 0.0790167823433876, 0.08297547698020935, 0.08449587970972061, 0.07959350198507309, 0.09043104946613312, 0.08141877502202988, 0.08803927898406982, 0.08706827461719513, 0.08665217459201813, 0.09197872877120972, 0.08171401917934418, 0.08180101215839386, 0.09197522699832916, 0.08688659965991974, 0.08188226819038391, 0.08117750287055969, 0.08385834842920303, 0.08086131513118744, 0.0781174749135971, 0.08448921889066696, 0.08363024890422821, 0.07951059192419052, 0.07973392307758331, 0.09014908224344254, 0.08058503270149231, 0.08884027600288391, 0.08152812719345093, 0.08157508820295334, 0.0853264108300209, 0.08143444359302521, 0.08769771456718445, 0.08074365556240082, 0.08496206998825073, 0.08341823518276215, 0.08580232411623001, 0.07883210480213165, 0.08528214693069458, 0.08497802913188934, 0.07543212175369263, 0.08124436438083649, 0.08442607522010803, 0.08556994050741196, 0.07823537290096283, 0.08225300908088684, 0.08074535429477692, 0.07778611779212952, 0.0819682776927948, 0.07751626521348953, 0.07988480478525162, 0.07807570695877075, 0.08184683322906494, 0.08074577152729034, 0.0802353024482727, 0.0826554074883461, 0.0868341326713562, 0.076393723487854, 0.07754341512918472, 0.08832982927560806, 0.08492887020111084, 0.0876326933503151, 0.07797962427139282, 0.07740683108568192, 0.07937086373567581, 0.08807146549224854, 0.07813536375761032, 0.0790797770023346, 0.0813986286520958, 0.07753980159759521, 0.07790906727313995, 0.08192605525255203, 0.08546888828277588, 0.07531315833330154, 0.08196047693490982, 0.07793432474136353, 0.08289146423339844, 0.07511986792087555, 0.08645068109035492, 0.08009230345487595, 0.08337870985269547, 0.0879620611667633, 0.08599197864532471, 0.08188652992248535, 0.08290524035692215, 0.08198203146457672, 0.07832604646682739, 0.07826332747936249, 0.08126938343048096, 0.0883479118347168, 0.08308953046798706, 0.08758163452148438, 0.08780054748058319, 0.08934146165847778, 0.08543842285871506, 0.08453202247619629, 0.07928281277418137, 0.087192103266716, 0.0787249207496643, 0.08252966403961182, 0.07916772365570068, 0.08579716086387634, 0.08561865985393524, 0.08086569607257843, 0.08029606938362122, 0.07862786948680878, 0.08021576702594757, 0.08273609727621078, 0.08864344656467438, 0.08017855882644653, 0.08197470754384995, 0.07632714509963989, 0.08706977963447571, 0.076617032289505, 0.08564412593841553, 0.08781693875789642, 0.08419203013181686, 0.08119907975196838, 0.08393160253763199, 0.07579949498176575, 0.08616504073143005, 0.08376875519752502, 0.08257532119750977, 0.08786960691213608, 0.08175501972436905, 0.08192025870084763, 0.0848701074719429, 0.08050552755594254, 0.08157716691493988, 0.08521511405706406, 0.07883282750844955, 0.08183011412620544, 0.07891515642404556, 0.08743844926357269, 0.08080683648586273, 0.08849721401929855, 0.08330219984054565, 0.08424663543701172, 0.0873553603887558, 0.08920661360025406, 0.08267250657081604, 0.0889672040939331, 0.08547486364841461, 0.08632881939411163, 0.08355661481618881, 0.0842360258102417, 0.07874580472707748, 0.07660914957523346, 0.0806460902094841, 0.07655270397663116, 0.07678143680095673, 0.08368192613124847, 0.0837247297167778, 0.080834299325943, 0.09039005637168884, 0.0781409740447998, 0.08442945778369904, 0.08165998756885529, 0.08411574363708496, 0.08628480136394501, 0.07175866514444351, 0.08727853000164032, 0.09227557480335236, 0.07453706860542297, 0.08514606952667236, 0.07497723400592804, 0.09177198261022568, 0.07375851273536682, 0.07989932596683502, 0.08098821341991425, 0.0799524337053299, 0.08534720540046692, 0.08567243814468384, 0.08189369738101959, 0.08395078778266907, 0.07937813550233841, 0.08902917802333832, 0.08631721138954163, 0.08010910451412201, 0.086420938372612, 0.07976396381855011, 0.07875314354896545, 0.08495637774467468, 0.08288523554801941, 0.07712610065937042, 0.08738559484481812, 0.07984816282987595, 0.07800963521003723, 0.08697845041751862, 0.08169947564601898, 0.08098747581243515, 0.08033741265535355, 0.07661009579896927, 0.07967983186244965, 0.08573126792907715, 0.08004890382289886, 0.07888756692409515, 0.08195041120052338, 0.08447979390621185, 0.08131737262010574, 0.07677793502807617, 0.07969801127910614, 0.08459608256816864, 0.08286527544260025, 0.08031818270683289, 0.08041978627443314, 0.08239740133285522, 0.08389981091022491, 0.07405003905296326, 0.07952941954135895, 0.09243285655975342, 0.08146841078996658, 0.08346396684646606, 0.08313734829425812, 0.07913178205490112, 0.08724594116210938, 0.07593575119972229, 0.07728048413991928, 0.08511373400688171, 0.07445639371871948, 0.07312780618667603, 0.08578566461801529, 0.08151385188102722, 0.0735037550330162, 0.0838385820388794, 0.08219407498836517, 0.08858700096607208, 0.08377771824598312, 0.0849415510892868, 0.0820847600698471, 0.08621294051408768, 0.07508920133113861, 0.08330444991588593, 0.08224591612815857, 0.07566249370574951, 0.08907324075698853, 0.08377331495285034, 0.0789249986410141, 0.07487289607524872, 0.08003885298967361, 0.07740644365549088, 0.08610512316226959, 0.09052775800228119, 0.08917365968227386, 0.07517899572849274, 0.08057210594415665, 0.08382834494113922, 0.08143214136362076, 0.07965604960918427, 0.07989974319934845, 0.08580676466226578, 0.08824823051691055, 0.07662428915500641, 0.08148709684610367, 0.07624858617782593, 0.08458257466554642, 0.08532833307981491, 0.07798434793949127, 0.08239387720823288, 0.08227844536304474, 0.0796898603439331, 0.09102267771959305, 0.08750210702419281, 0.07983897626399994, 0.07804004102945328, 0.07899952679872513, 0.07562552392482758, 0.08203588426113129, 0.08429958671331406, 0.08707359433174133, 0.08605837821960449, 0.08667513728141785, 0.07678911089897156, 0.07591764628887177, 0.07761834561824799, 0.08006767928600311, 0.0831686407327652, 0.07908783107995987, 0.07566766440868378, 0.0882905125617981, 0.08641579747200012, 0.08545006811618805, 0.08089158684015274, 0.07515501976013184, 0.08061107993125916, 0.07703765481710434, 0.0811280906200409, 0.07530492544174194, 0.0927797183394432, 0.08018478751182556, 0.08200466632843018, 0.07921580970287323, 0.08183714747428894, 0.09065897762775421, 0.08132900297641754, 0.0863465964794159, 0.07944048941135406, 0.08217422664165497, 0.0802590399980545, 0.08073686808347702, 0.08570104092359543, 0.08035089820623398, 0.07620597630739212, 0.07434959709644318, 0.08630163967609406, 0.07844563573598862, 0.07997677475214005, 0.07906882464885712, 0.08817976713180542, 0.0818348154425621, 0.08004170656204224, 0.08641904592514038, 0.08448922634124756, 0.08047666400671005, 0.08216600120067596, 0.07911860942840576, 0.08101038634777069, 0.08081144094467163, 0.083893783390522, 0.08122061938047409, 0.08518779277801514, 0.08065181970596313, 0.07939131557941437, 0.07877329736948013, 0.08551216125488281, 0.07452189922332764, 0.08112643659114838, 0.08691401779651642, 0.08222492039203644, 0.08536362648010254, 0.08611349761486053, 0.08326514065265656, 0.08291596174240112, 0.07526122033596039, 0.08235888183116913, 0.07327128946781158, 0.08601625263690948, 0.07835128903388977, 0.07585282623767853, 0.06951113045215607, 0.08103571832180023, 0.08719470351934433, 0.0844605565071106, 0.08313509821891785, 0.07834942638874054, 0.07910770177841187, 0.0823596864938736, 0.07396755367517471, 0.0904499739408493, 0.07657486945390701, 0.08378764241933823, 0.07760518789291382, 0.08110235631465912, 0.08518080413341522, 0.09011507034301758, 0.08798599243164062, 0.08213945478200912, 0.07636576145887375, 0.08599181473255157, 0.08494970202445984, 0.08300614356994629, 0.07420286536216736, 0.08472000807523727, 0.08435006439685822, 0.08063388615846634, 0.08196540921926498, 0.07975271344184875, 0.08582333475351334, 0.08281847089529037, 0.07177263498306274, 0.07513776421546936, 0.08137825131416321, 0.08186833560466766, 0.084309883415699, 0.0863676369190216, 0.07506642490625381, 0.0881161242723465, 0.07716378569602966, 0.08511948585510254, 0.07889010012149811, 0.08046309649944305, 0.07191282510757446, 0.08006294816732407, 0.08643368631601334, 0.084012970328331, 0.08399375528097153, 0.0796322152018547, 0.08307436853647232, 0.08809135854244232, 0.0841345265507698, 0.08793742209672928, 0.07643605768680573, 0.08702898025512695, 0.09061124920845032, 0.08734303712844849, 0.07684752345085144, 0.08236449956893921, 0.08369775116443634, 0.08377912640571594, 0.08345712721347809, 0.07464173436164856, 0.07438650727272034, 0.08164501190185547, 0.08520575612783432, 0.07872680574655533, 0.08431132137775421, 0.08027279376983643, 0.08454710245132446, 0.08396713435649872, 0.07561402767896652, 0.08508456498384476, 0.0889677107334137, 0.07327339053153992, 0.07634497433900833, 0.08408543467521667, 0.07683079689741135, 0.08502469956874847, 0.08735345304012299, 0.08590532094240189, 0.08142504096031189, 0.0822889506816864, 0.09037383645772934, 0.08351260423660278, 0.07278752326965332, 0.08251435309648514, 0.08594264090061188, 0.0809883400797844, 0.08546318113803864, 0.08400847017765045, 0.07559110224246979, 0.07872520387172699, 0.08322965353727341, 0.08504538238048553, 0.07989534735679626, 0.08339956402778625, 0.07229714095592499, 0.07981261610984802, 0.08422151207923889, 0.08284250646829605, 0.08097285032272339, 0.07763654738664627, 0.08215342462062836, 0.07614989578723907, 0.07667693495750427, 0.07917945086956024, 0.08980502188205719, 0.07444476336240768, 0.08294051885604858, 0.07903456687927246, 0.0805743932723999, 0.0767751932144165, 0.08507378399372101, 0.08940454572439194, 0.07612213492393494, 0.0851755142211914, 0.08070813119411469, 0.08348797261714935, 0.08495238423347473, 0.08741308748722076, 0.08290686458349228, 0.08311101794242859, 0.07917937636375427, 0.07867264747619629, 0.07546176016330719, 0.07616505026817322, 0.08286707103252411, 0.08373875916004181, 0.08288674056529999, 0.0803704708814621, 0.07805982232093811, 0.08252870291471481, 0.0794929563999176, 0.07933066785335541, 0.0739482194185257, 0.08353772014379501, 0.08307547122240067, 0.074229896068573, 0.08336403220891953, 0.07991991937160492, 0.0873807966709137, 0.07823082059621811, 0.08144399523735046, 0.08374176919460297, 0.08214778453111649, 0.08118041604757309, 0.08050959557294846, 0.0823201984167099, 0.08082342147827148, 0.08524012565612793, 0.0850806012749672, 0.07901996374130249, 0.08349566161632538, 0.0866086483001709, 0.07974611222743988, 0.08194626122713089, 0.0863872766494751, 0.0881945788860321, 0.08341237902641296, 0.07850101590156555, 0.08593831956386566, 0.0799599289894104, 0.08084208518266678, 0.08141603320837021, 0.08008688688278198, 0.08125101774930954, 0.07771104574203491, 0.07771183550357819, 0.08620722591876984, 0.08342179656028748, 0.074442058801651, 0.07885751128196716, 0.0748143270611763, 0.08192349225282669, 0.08012788742780685, 0.07578535377979279, 0.082201287150383, 0.0857488214969635, 0.07780838012695312, 0.08244024217128754, 0.07810404151678085, 0.07787446677684784, 0.0799044743180275, 0.07648899406194687, 0.08188571035861969, 0.08295285701751709, 0.09023736417293549, 0.08357490599155426, 0.08033667504787445, 0.08100824058055878, 0.08656662702560425, 0.08019457757472992, 0.0750652402639389, 0.09746338427066803, 0.09190136939287186, 0.08399307727813721, 0.08537675440311432, 0.07906602323055267, 0.07794803380966187, 0.08140844851732254, 0.0833987295627594, 0.08361607044935226, 0.07873901724815369, 0.07907760143280029, 0.07685735076665878, 0.08709080517292023, 0.07935553044080734, 0.08479534089565277, 0.07897152006626129, 0.08063121140003204, 0.08016751706600189, 0.08615334331989288, 0.0789865255355835, 0.08418362587690353, 0.07872940599918365, 0.0855451375246048, 0.07993602752685547, 0.08718036860227585, 0.07598753273487091, 0.08423010259866714, 0.07967185974121094, 0.08321356028318405, 0.08096823841333389, 0.07663455605506897, 0.08048336207866669, 0.07866677641868591, 0.07981345057487488, 0.08473466336727142, 0.07996813952922821, 0.08440801501274109, 0.08402474224567413, 0.08253446221351624, 0.0797959417104721, 0.07367902994155884, 0.08511411398649216, 0.08379504829645157, 0.09208208322525024, 0.0829877257347107, 0.0785098522901535, 0.07891301810741425, 0.0851927101612091, 0.09042629599571228, 0.08068612962961197, 0.08242814242839813, 0.08509743213653564, 0.08728180825710297, 0.08150462806224823, 0.08880384266376495, 0.08466152101755142, 0.08759815245866776, 0.08026403188705444, 0.0780036449432373, 0.08741644024848938, 0.07485412061214447, 0.0774579644203186, 0.08660595118999481, 0.08950871974229813, 0.08147664368152618, 0.08005303144454956, 0.08300120383501053, 0.08591732382774353, 0.081572987139225, 0.09091082215309143, 0.09129075706005096, 0.08246259391307831, 0.08158473670482635, 0.07648217678070068, 0.0806940346956253, 0.07918787002563477, 0.08697617053985596, 0.07786466926336288, 0.0752359926700592, 0.07708406448364258, 0.08123715221881866, 0.08965525031089783, 0.09112142026424408, 0.07930153608322144, 0.08334450423717499, 0.07562201470136642, 0.08146527409553528, 0.08485963940620422, 0.08192256093025208, 0.08443191647529602, 0.09101679921150208, 0.0851273164153099, 0.07900911569595337, 0.08542819321155548, 0.07540478557348251, 0.08138421177864075, 0.07676061242818832, 0.08074280619621277, 0.07385179400444031, 0.08519762754440308, 0.08840634673833847, 0.08072777092456818, 0.0831616073846817, 0.08543389290571213, 0.07590121775865555, 0.07993170619010925, 0.08783753961324692, 0.07543444633483887, 0.0822853296995163, 0.07210130989551544, 0.08336776494979858, 0.08002720773220062, 0.08443515747785568, 0.08086077868938446, 0.08272919058799744, 0.08388102054595947, 0.09144277125597, 0.08194334805011749, 0.08222361654043198, 0.08284291625022888, 0.07852878421545029, 0.0764966607093811, 0.08331473171710968, 0.07625530660152435, 0.08397264778614044, 0.08293629437685013, 0.08471449464559555, 0.088788241147995, 0.07899337261915207, 0.07384394854307175, 0.0882464051246643, 0.08497758209705353, 0.08434346318244934, 0.08428254723548889, 0.07249750196933746, 0.08174198865890503, 0.07671360671520233, 0.08015494048595428, 0.07540415227413177, 0.0772235170006752, 0.08708500862121582, 0.09144921600818634, 0.08069629967212677, 0.08840786665678024, 0.08626900613307953, 0.08297180384397507, 0.08404972404241562, 0.08447127044200897, 0.08106055855751038, 0.08474498987197876, 0.08607755601406097, 0.08182454109191895, 0.07995767891407013, 0.08391211926937103, 0.0857258141040802, 0.08631884306669235, 0.08009754121303558, 0.08132799714803696, 0.08762934058904648, 0.07524240016937256, 0.08295603841543198, 0.0816265344619751, 0.08369371294975281, 0.08423551917076111, 0.08508466929197311, 0.08150194585323334, 0.08659288287162781, 0.07827046513557434, 0.07858976721763611, 0.08661247789859772, 0.08110173046588898, 0.08301755785942078, 0.0810956284403801, 0.08425094187259674, 0.0813203752040863, 0.07674933969974518, 0.07614099979400635, 0.08759765326976776, 0.07841750234365463, 0.08508296310901642, 0.07377606630325317, 0.07474984228610992, 0.08630076050758362, 0.08385998755693436, 0.08146397769451141, 0.08239837735891342, 0.08084402978420258, 0.07613196969032288, 0.08318319916725159, 0.0920024961233139, 0.08324095606803894, 0.08912645280361176, 0.08303746581077576, 0.0771479457616806, 0.07962146401405334, 0.08030277490615845, 0.0808478370308876, 0.07668142020702362, 0.0789232924580574, 0.07623368501663208, 0.07606886327266693, 0.07933254539966583, 0.07544800639152527, 0.08116524666547775, 0.07736697793006897, 0.08408993482589722, 0.08908163756132126, 0.07918977737426758, 0.08001376688480377, 0.08527369797229767, 0.07728053629398346, 0.0875151976943016, 0.08371216058731079, 0.08298072218894958, 0.0917908251285553, 0.08195159584283829, 0.0832958072423935, 0.08411189168691635, 0.07474328577518463, 0.08454239368438721, 0.07965011894702911, 0.08361802995204926, 0.07796481251716614, 0.0765974298119545, 0.07674635946750641, 0.08289992809295654, 0.0897812694311142, 0.07863235473632812, 0.08240261673927307, 0.08683335781097412, 0.0781671553850174, 0.07852105051279068, 0.07625716924667358, 0.07786144316196442, 0.08363009244203568, 0.08113671839237213, 0.08317627757787704, 0.08409462869167328, 0.07908567786216736, 0.08192421495914459, 0.07909362018108368, 0.08030970394611359, 0.0828709602355957, 0.08859621733427048, 0.08606982231140137, 0.08083468675613403, 0.07961767166852951, 0.07671608030796051, 0.08426384627819061, 0.08453837037086487, 0.0819210559129715, 0.07693439722061157, 0.07689161598682404, 0.08808610588312149, 0.08542235195636749, 0.07783576101064682, 0.08195950090885162, 0.07683995366096497, 0.0852232277393341, 0.07766669988632202, 0.07509007304906845, 0.07724680006504059, 0.08210988342761993, 0.08277019113302231, 0.07920053601264954, 0.0827380120754242, 0.08648815751075745, 0.08413933962583542, 0.08654958009719849, 0.08203244209289551, 0.08817727118730545, 0.08375842869281769, 0.0793626606464386, 0.07930197566747665, 0.08271174877882004, 0.07810674607753754, 0.07865998893976212, 0.08372475951910019, 0.07549186795949936, 0.08528701961040497, 0.08049234002828598, 0.07973010092973709, 0.07562480121850967, 0.0806465819478035, 0.07697319984436035, 0.08755765110254288, 0.07593417167663574, 0.07961907982826233, 0.08447816967964172, 0.08005300164222717, 0.08831912279129028, 0.08151115477085114, 0.08010770380496979, 0.08053787052631378, 0.07747457921504974, 0.07408982515335083, 0.08185280859470367, 0.08657117187976837, 0.07534398138523102, 0.08926913887262344, 0.08868415653705597, 0.07804477214813232, 0.08305348455905914, 0.09031599760055542, 0.07861395180225372, 0.0787852555513382, 0.08131082355976105, 0.0877659022808075, 0.07796695083379745, 0.08050009608268738, 0.08710281550884247, 0.08235743641853333, 0.07928670942783356, 0.08302056789398193, 0.08681543171405792, 0.08151323348283768, 0.08391126990318298, 0.09359433501958847, 0.08176562190055847, 0.07958793640136719, 0.08917102217674255, 0.08259712904691696, 0.0723758190870285, 0.08043175935745239, 0.08383385092020035, 0.07287578284740448, 0.08280764520168304, 0.08104008436203003, 0.08907001465559006, 0.08277997374534607, 0.07977716624736786, 0.0818534791469574, 0.08343593031167984, 0.08454184979200363, 0.07386654615402222, 0.08155026286840439, 0.08455735445022583, 0.08208949118852615, 0.08451956510543823, 0.081122487783432, 0.0793641060590744, 0.08217346668243408, 0.08214403688907623, 0.08842675387859344, 0.0832623839378357, 0.08634990453720093, 0.08010005950927734, 0.08939242362976074, 0.07653859257698059, 0.07954144477844238, 0.07745411247015, 0.08082212507724762, 0.08636394888162613, 0.07656148821115494, 0.07866248488426208, 0.08102446049451828, 0.08067004382610321, 0.08105242997407913, 0.07817821204662323, 0.08461007475852966, 0.08071891218423843, 0.07562732696533203, 0.08572963625192642, 0.07794882357120514, 0.08115768432617188, 0.08507708460092545, 0.08421057462692261, 0.08209968358278275, 0.08243469893932343, 0.08242393285036087, 0.07014963030815125, 0.08135524392127991, 0.08282577246427536, 0.07760514318943024, 0.08191412687301636, 0.08115830272436142, 0.08026093244552612, 0.08096054941415787, 0.0812830924987793, 0.09108627587556839, 0.08301281929016113, 0.08750465512275696, 0.07455704361200333, 0.08279690146446228, 0.08746016025543213, 0.08481346070766449, 0.0777844488620758, 0.0759592056274414, 0.08700070530176163, 0.080195352435112, 0.08380391448736191, 0.07987482845783234, 0.0811598151922226, 0.08249562233686447, 0.08425849676132202, 0.08898672461509705, 0.09012360870838165, 0.08335046470165253, 0.08392849564552307, 0.08100008219480515, 0.07768164575099945, 0.08001802861690521, 0.08629408478736877, 0.08420436084270477, 0.08146218955516815, 0.07626816630363464, 0.08391495048999786, 0.0883987694978714, 0.07669292390346527, 0.0799737274646759, 0.0772247165441513, 0.07362833619117737, 0.07926223427057266, 0.08463265001773834, 0.07886744290590286, 0.07997359335422516, 0.08564914762973785, 0.08175838738679886, 0.07813282310962677, 0.08161118626594543, 0.08135485649108887, 0.08707516640424728, 0.08836384117603302, 0.08959983289241791, 0.07931795716285706, 0.0919339656829834, 0.08419664949178696, 0.08443161845207214, 0.08156286925077438, 0.07678253948688507, 0.08864045888185501, 0.07849664986133575, 0.08206849545240402, 0.07881060987710953, 0.078356072306633, 0.08197150379419327, 0.08184660971164703, 0.08162997663021088, 0.07666809111833572, 0.08265546709299088, 0.08483921736478806, 0.07722364366054535, 0.07958699762821198, 0.08221254497766495, 0.08297809958457947, 0.08009932935237885, 0.08004924654960632, 0.08354784548282623, 0.08791544288396835, 0.07780729234218597, 0.08564130961894989, 0.08331124484539032, 0.07590559124946594, 0.07972598820924759, 0.07986045628786087, 0.08594607561826706, 0.08697611838579178, 0.09388178586959839, 0.08145418763160706, 0.07753248512744904, 0.07746312022209167, 0.08451652526855469, 0.08451274037361145, 0.08390986919403076, 0.07893253862857819, 0.08140943199396133, 0.08469513058662415, 0.076468326151371, 0.08221893012523651, 0.0853748470544815, 0.07968689501285553, 0.08663024008274078, 0.08147772401571274, 0.08889394998550415, 0.08692215383052826, 0.08389108628034592, 0.08888721466064453, 0.08133506774902344, 0.08286964893341064, 0.07169204950332642, 0.07177508622407913, 0.0861574038863182, 0.08330848813056946, 0.07453469932079315, 0.07321996986865997, 0.08240343630313873, 0.0833882987499237, 0.08464127779006958, 0.08462388813495636, 0.08558778464794159, 0.08756409585475922, 0.08427773416042328, 0.07976769655942917, 0.08446300029754639, 0.07971379160881042, 0.07998668402433395, 0.08306369185447693, 0.07786242663860321, 0.08283452689647675, 0.08558119833469391, 0.07815621048212051, 0.09196692705154419, 0.08182822167873383, 0.07663221657276154, 0.08273504674434662, 0.08170460164546967, 0.08545137941837311, 0.08256841450929642, 0.07778841257095337, 0.08305840194225311, 0.08478689193725586, 0.08025568723678589, 0.07886569201946259, 0.0787261575460434, 0.08142392337322235, 0.07348868250846863, 0.08228476345539093, 0.08559660613536835, 0.07977606356143951, 0.08599460124969482, 0.08066493272781372, 0.0764215961098671, 0.08190788328647614, 0.08311060070991516, 0.08246219903230667, 0.08620880544185638, 0.08459247648715973, 0.08673778176307678, 0.08150725066661835, 0.08534131199121475, 0.07692329585552216, 0.08183209598064423, 0.08300222456455231, 0.07995393872261047, 0.0774189829826355, 0.0884041041135788, 0.0829833522439003, 0.08139318972826004, 0.08139871060848236, 0.08823727071285248, 0.0776640921831131, 0.08092436194419861, 0.07556653022766113, 0.08599260449409485, 0.0744614452123642, 0.08158659934997559, 0.07952592521905899, 0.08175511658191681, 0.08209389448165894, 0.08178076148033142, 0.08749117702245712, 0.07779298722743988, 0.07964293658733368, 0.08166108280420303, 0.08387959003448486, 0.081275075674057, 0.08043648302555084, 0.0816582441329956, 0.07653211057186127, 0.07560867071151733, 0.07807841897010803, 0.0760934054851532, 0.0885649025440216, 0.08269190788269043, 0.0873626321554184, 0.06973620504140854, 0.07754360884428024, 0.08199957013130188, 0.08418288826942444, 0.0852956771850586, 0.08278122544288635, 0.07975837588310242, 0.07791025191545486, 0.0786738395690918, 0.0830664336681366, 0.08136121928691864, 0.08596169203519821, 0.08198267221450806, 0.07607271522283554, 0.08157230913639069, 0.0801854357123375, 0.07984708249568939, 0.08217284828424454, 0.08499381691217422, 0.08872787654399872, 0.08788195252418518, 0.08606988936662674, 0.07861702144145966, 0.08289570361375809, 0.08596983551979065, 0.08094726502895355, 0.08889906108379364, 0.0897241160273552, 0.08601422607898712, 0.0813402384519577, 0.08107764273881912, 0.08168105781078339, 0.08048711717128754, 0.08536982536315918, 0.08078008890151978, 0.09121733903884888, 0.08912032097578049, 0.07992272078990936, 0.08521353453397751, 0.08408588171005249, 0.07192923873662949, 0.07882040739059448, 0.07711748778820038, 0.08099664747714996, 0.077767014503479, 0.07777508348226547, 0.07964718341827393, 0.08107070624828339, 0.09137453138828278, 0.08931118994951248, 0.08437326550483704, 0.07757742702960968, 0.08048927783966064, 0.08088972419500351, 0.0807100310921669, 0.08588194847106934, 0.0752054825425148, 0.07764822244644165, 0.08531570434570312, 0.07699626684188843, 0.08358906209468842, 0.08210280537605286, 0.07600653916597366, 0.08290216326713562, 0.09450150281190872, 0.07718570530414581, 0.07857363671064377, 0.08161056786775589, 0.08273717761039734, 0.08179035782814026, 0.07765963673591614, 0.08644340932369232, 0.08474542945623398, 0.08795040845870972, 0.08939865231513977, 0.07274887710809708, 0.0790417343378067, 0.08686619251966476, 0.0767868161201477, 0.08772915601730347, 0.0828021839261055, 0.08279134333133698, 0.0818193331360817, 0.07702124118804932, 0.08011092245578766, 0.07538540661334991, 0.07919736206531525, 0.07785013318061829, 0.08433490991592407, 0.08311763405799866, 0.08265647292137146, 0.0821390151977539, 0.08353310078382492, 0.0835442841053009, 0.09273524582386017, 0.07866677641868591, 0.08434098958969116, 0.08154474198818207, 0.08278582990169525, 0.08414619415998459, 0.07666962593793869, 0.07829159498214722, 0.08237337321043015, 0.08523673564195633, 0.0843527764081955, 0.08971640467643738, 0.08511783927679062, 0.09030792117118835, 0.07722241431474686, 0.08755987882614136, 0.08580932021141052, 0.08694398403167725, 0.081753671169281, 0.08374922722578049, 0.08654817938804626, 0.09084339439868927, 0.07493352890014648, 0.07656379789113998, 0.07974807173013687, 0.0828375518321991, 0.08516353368759155, 0.07837128639221191, 0.08687061071395874, 0.08699650317430496, 0.07961361110210419, 0.07384271919727325, 0.08305355161428452, 0.0848274827003479, 0.08078113198280334, 0.08384053409099579, 0.08482626080513, 0.0819096565246582, 0.08366462588310242, 0.08363549411296844, 0.07336845248937607, 0.07950186729431152, 0.07889674603939056, 0.07600398361682892, 0.07828038930892944, 0.08428080379962921, 0.08446471393108368, 0.07244688272476196, 0.07932854443788528, 0.08717948943376541, 0.07911117374897003, 0.08047565817832947, 0.08504259586334229, 0.07723099738359451, 0.08273647725582123, 0.08375228196382523, 0.08170592784881592, 0.08151339739561081, 0.0832144170999527, 0.0793670192360878, 0.08909331262111664, 0.08536061644554138, 0.08958525955677032, 0.0795087218284607, 0.08040790259838104, 0.08731422573328018, 0.0830569863319397, 0.08029422163963318, 0.08041732758283615, 0.07794781774282455, 0.09010578691959381, 0.08092385530471802, 0.08211749792098999, 0.08326683193445206, 0.08932547271251678, 0.07530894875526428, 0.07218348979949951, 0.08106742799282074, 0.07484444230794907, 0.09018971771001816, 0.07873071730136871, 0.08422554284334183, 0.07727311551570892, 0.07313380390405655, 0.08168322592973709, 0.08038419485092163, 0.08253775537014008, 0.09120559692382812, 0.0768766850233078, 0.08465100079774857, 0.08337034285068512, 0.09195918589830399, 0.0765610933303833, 0.07537969201803207, 0.08317136764526367, 0.0795898512005806, 0.0856211930513382, 0.08328141272068024, 0.0822560116648674, 0.0888168066740036, 0.08584926277399063, 0.08366499841213226, 0.07169460505247116, 0.08190616965293884, 0.08603543043136597, 0.08323308825492859, 0.07612457126379013, 0.07793853431940079, 0.07860331982374191, 0.07773341238498688, 0.07846701145172119, 0.07854580879211426, 0.08397451788187027, 0.08755523711442947, 0.08111502975225449, 0.0780501514673233, 0.081293985247612, 0.08024843782186508, 0.08441518247127533, 0.08939129114151001, 0.08633368462324142, 0.07895514369010925, 0.08757112920284271, 0.09255865216255188, 0.08233551681041718, 0.08112050592899323, 0.08645131438970566, 0.0825347751379013, 0.0802566409111023, 0.08706635236740112, 0.07904567569494247, 0.07663212716579437, 0.08775870501995087, 0.07773593813180923, 0.07700614631175995, 0.08887264132499695, 0.08345884084701538, 0.07829277217388153, 0.07863952219486237, 0.08018888533115387, 0.0829729214310646, 0.08131683617830276, 0.08310788869857788, 0.07999996840953827, 0.07747938483953476, 0.0809078961610794, 0.0765557736158371, 0.07814522087574005, 0.08626077324151993, 0.08739656209945679, 0.08581593632698059, 0.08014455437660217, 0.08336873352527618, 0.08890379220247269, 0.08019007742404938, 0.0833878368139267, 0.0841195285320282, 0.08956173062324524, 0.07857880741357803, 0.07974228262901306, 0.07777415961027145, 0.08193191885948181, 0.08248713612556458, 0.07809771597385406, 0.07522059977054596, 0.08291912823915482, 0.08262144029140472, 0.07797887921333313, 0.07710455358028412, 0.08331182599067688, 0.0849699079990387, 0.08112287521362305, 0.08446362614631653, 0.07601147890090942, 0.0976763367652893, 0.08710132539272308, 0.08790571987628937, 0.08568303287029266, 0.08764994889497757, 0.07677555084228516, 0.0847896933555603, 0.08643747121095657, 0.08283931016921997, 0.07818710803985596, 0.07926209270954132, 0.07778467983007431, 0.08702312409877777, 0.09417277574539185, 0.08492615818977356, 0.0762421190738678, 0.08294449746608734, 0.0846232995390892, 0.08030059933662415, 0.08243632316589355, 0.09000992774963379, 0.08066047728061676, 0.08402109146118164, 0.07624292373657227, 0.08201462030410767, 0.0825565904378891, 0.08417797833681107, 0.08231339603662491, 0.08537402749061584, 0.08044596761465073, 0.07889489084482193, 0.08269436657428741, 0.08235988765954971, 0.08499343693256378, 0.09067334234714508, 0.08422836661338806, 0.07802370935678482, 0.09188377112150192, 0.08708953857421875, 0.08306316286325455, 0.08354996144771576, 0.07905679941177368, 0.08276988565921783, 0.06913676857948303, 0.0832998976111412, 0.07764383405447006, 0.0811556950211525, 0.0782826840877533, 0.079043447971344, 0.07567036151885986, 0.0843130499124527, 0.085446298122406, 0.08031289279460907, 0.0846438854932785, 0.0891965925693512, 0.08349257707595825, 0.07850225269794464, 0.084737129509449, 0.08580976724624634, 0.08233217895030975, 0.073798269033432, 0.07343712449073792, 0.08313007652759552, 0.0825776606798172, 0.07821322977542877, 0.08869512379169464, 0.07892052084207535, 0.08202005177736282, 0.08151483535766602, 0.08491876721382141, 0.08437225967645645, 0.08932242542505264, 0.07914502173662186, 0.0876743495464325, 0.07987239956855774, 0.07271702587604523, 0.08727233856916428, 0.08171910047531128, 0.08629399538040161, 0.08040659874677658, 0.086310014128685, 0.0811726301908493, 0.07961827516555786, 0.07917845249176025, 0.0822887048125267, 0.08129215240478516, 0.08771717548370361, 0.08451908826828003, 0.08206471055746078, 0.08362485468387604, 0.08096837252378464, 0.08003751188516617, 0.07891078293323517, 0.08672458678483963, 0.08443547785282135, 0.08394859731197357, 0.08847914636135101, 0.08499009907245636, 0.07223212718963623, 0.07847849279642105, 0.08582690358161926, 0.07894046604633331, 0.09423282742500305, 0.08506771177053452, 0.08395165205001831, 0.08550463616847992, 0.08693080395460129, 0.08727389574050903, 0.07459185272455215, 0.08267675340175629, 0.08087576925754547, 0.088386669754982, 0.08437483012676239, 0.07014373689889908, 0.08200444281101227, 0.08036478608846664, 0.07825322449207306, 0.07531829178333282, 0.08326233178377151, 0.0865287333726883, 0.08134274184703827, 0.08354601263999939, 0.08216983824968338, 0.07813413441181183, 0.08142805844545364, 0.07962654531002045, 0.08170118927955627, 0.08353878557682037, 0.06979036331176758, 0.08817839622497559, 0.07897939532995224, 0.0764600858092308, 0.0777319073677063, 0.0824202224612236, 0.08397947251796722, 0.08801886439323425, 0.0843849778175354, 0.08092635124921799, 0.08165992051362991, 0.08938171714544296, 0.08353486657142639, 0.08043979108333588, 0.08454502373933792, 0.07791031897068024, 0.0870266854763031, 0.08538410812616348, 0.08755066990852356, 0.07861388474702835, 0.07771039009094238, 0.0807281881570816, 0.07976111024618149, 0.08126743137836456, 0.07811532914638519, 0.08084358274936676, 0.08333414047956467, 0.08068148791790009, 0.08083195984363556, 0.08786191791296005, 0.08427542448043823, 0.0796879455447197, 0.07858676463365555, 0.08487747609615326, 0.08363068103790283, 0.08960264176130295, 0.0892161875963211, 0.08099773526191711, 0.08235085010528564, 0.0884975865483284, 0.07689663022756577, 0.07965239882469177, 0.07301013171672821, 0.08865723013877869, 0.07277986407279968, 0.08269375562667847, 0.08099351078271866, 0.08400103449821472, 0.07971619069576263, 0.08014979958534241, 0.08358357846736908, 0.08153261244297028, 0.08615025132894516, 0.08528885245323181, 0.07600196450948715, 0.08252207189798355, 0.08649537712335587, 0.07961337268352509, 0.07783880084753036, 0.08851979672908783, 0.07891736179590225, 0.08825293183326721, 0.07324764132499695, 0.0769403725862503, 0.08369994908571243, 0.08652953058481216, 0.08495031297206879, 0.08903344720602036, 0.08947114646434784, 0.08648794889450073, 0.07951469719409943, 0.08269448578357697, 0.08036495745182037, 0.07832450419664383, 0.08370338380336761, 0.0857212021946907, 0.08173349499702454, 0.07605502754449844, 0.08479033410549164, 0.08873981237411499, 0.0793159082531929, 0.08158671855926514, 0.08243505656719208, 0.07628621906042099, 0.07543247938156128, 0.08307589590549469, 0.07447909563779831, 0.084776870906353, 0.07395558059215546, 0.08526715636253357, 0.0811050534248352, 0.0749664381146431, 0.08591050654649734, 0.07798733562231064, 0.0789613202214241, 0.07944275438785553, 0.0794232189655304, 0.07413586229085922, 0.09076876193284988, 0.08143171668052673, 0.08051872253417969, 0.08255928754806519, 0.0864083394408226, 0.08234614133834839, 0.08438810706138611, 0.08113791048526764, 0.08287559449672699, 0.08270739018917084, 0.08103744685649872, 0.07771461457014084, 0.08421042561531067, 0.08350013941526413, 0.08434663712978363, 0.08134245872497559, 0.08502033352851868, 0.0818236842751503, 0.0744754746556282, 0.0812528133392334, 0.0887240543961525, 0.08175529539585114, 0.07892325520515442, 0.08421919494867325, 0.08362428843975067, 0.08515071123838425, 0.07592388242483139, 0.07376286387443542, 0.07809804379940033, 0.08193030953407288, 0.0777682438492775, 0.07600763440132141, 0.0772903710603714, 0.08028110861778259, 0.07831967622041702, 0.08323584496974945, 0.08566057682037354, 0.08861971646547318, 0.08502459526062012, 0.08673234283924103, 0.07582928240299225, 0.0873611718416214, 0.0823407769203186, 0.08050769567489624, 0.08702483773231506, 0.07873591780662537, 0.08396556973457336, 0.08248228579759598, 0.07779715955257416, 0.08442039787769318, 0.08530905842781067, 0.07274968922138214, 0.08059647679328918, 0.08362515270709991, 0.07404051721096039, 0.08386854827404022, 0.0786595270037651, 0.08122196048498154, 0.07745361328125, 0.08013802766799927, 0.08146496117115021, 0.08435937017202377, 0.07877996563911438, 0.08383914083242416, 0.08250712603330612, 0.08595740795135498, 0.07882075011730194, 0.07819667458534241, 0.07497449219226837, 0.08108590543270111, 0.08181148767471313, 0.08575867116451263, 0.07280828803777695, 0.08862295746803284, 0.08540578186511993, 0.0797395259141922, 0.08356882631778717, 0.08323740214109421, 0.08228238672018051, 0.08067931234836578, 0.0804937332868576, 0.08616066724061966, 0.07595144957304001, 0.08572036027908325, 0.09014487266540527, 0.081918865442276, 0.07566337287425995, 0.08434087038040161, 0.07241389155387878, 0.08255119621753693, 0.0804678350687027, 0.07761106640100479, 0.08517414331436157, 0.08476565778255463, 0.0792970061302185, 0.08791390806436539, 0.08571257442235947, 0.08640901744365692, 0.07840727269649506, 0.08779649436473846, 0.08421628177165985, 0.08393070101737976, 0.08565332740545273, 0.08057796210050583, 0.0801093727350235, 0.07723135501146317, 0.09187503159046173, 0.08323654532432556, 0.07321684807538986, 0.07628336548805237, 0.08577311038970947, 0.08206328749656677, 0.07731010764837265, 0.0861416831612587, 0.07465802133083344, 0.08617495000362396, 0.07413387298583984, 0.07442444562911987, 0.08490866422653198, 0.07265685498714447, 0.0867132842540741, 0.0891752690076828, 0.08577369898557663, 0.0829935148358345, 0.08068161457777023, 0.08711115270853043, 0.08015850186347961, 0.08044783771038055, 0.08488285541534424, 0.08808249980211258, 0.07884155958890915, 0.08237772434949875, 0.07728599011898041, 0.0851622223854065, 0.09261218458414078, 0.08179514110088348, 0.07244381308555603, 0.08626147359609604, 0.07945283502340317, 0.08157104253768921, 0.07472372055053711, 0.08018144965171814, 0.09130041301250458, 0.07774800062179565, 0.0809546411037445, 0.08067325502634048, 0.08789972215890884, 0.0799938291311264, 0.08247103542089462, 0.08323270082473755, 0.07246381044387817, 0.08023103326559067, 0.07928106933832169, 0.08329819142818451, 0.08984549343585968, 0.09170487523078918, 0.08062957227230072, 0.09075608104467392, 0.07729261368513107, 0.08057918399572372, 0.08267570286989212, 0.07876987755298615, 0.07591591775417328, 0.08283743262290955, 0.08269518613815308, 0.08557857573032379, 0.07967976480722427, 0.08256161212921143, 0.07997215539216995, 0.08467397093772888, 0.07156264781951904, 0.08683022856712341, 0.08849841356277466, 0.08497703075408936, 0.08244304358959198, 0.08067045360803604, 0.07276886701583862, 0.0860077440738678, 0.0842062309384346, 0.0862915962934494, 0.08303175866603851, 0.08094176650047302, 0.07408377528190613, 0.08414812386035919, 0.07875926792621613, 0.08511568605899811, 0.08581005036830902, 0.08439764380455017, 0.08120715618133545, 0.07517293840646744, 0.09502293169498444, 0.0772377997636795, 0.0817299634218216, 0.09339341521263123, 0.07962293922901154, 0.08863571286201477, 0.08443546295166016, 0.08497504889965057, 0.07697232067584991, 0.07640886306762695, 0.0696885883808136, 0.0852309986948967, 0.07992491126060486, 0.08228155970573425, 0.09016484767198563, 0.08454073220491409, 0.09170487523078918, 0.07635587453842163, 0.08422745764255524, 0.07687002420425415, 0.0905468612909317, 0.08405807614326477, 0.09157420694828033, 0.08237191289663315, 0.0824534147977829, 0.08354875445365906, 0.08220893144607544, 0.08494648337364197, 0.07878020405769348, 0.08301100134849548, 0.08481311798095703, 0.08002857863903046, 0.07569604367017746, 0.08632588386535645, 0.0857485830783844, 0.08032569289207458, 0.07446320354938507, 0.07912657409906387, 0.08753596246242523, 0.0862116813659668, 0.08223909139633179, 0.08699609339237213, 0.08223811537027359, 0.085922971367836, 0.08387741446495056, 0.08523746579885483, 0.07727634906768799, 0.0914398580789566, 0.07262818515300751, 0.08401603996753693, 0.08459740877151489, 0.0808643326163292, 0.08578066527843475, 0.09178461879491806, 0.08885069191455841, 0.09381479769945145, 0.07489751279354095, 0.08405013382434845, 0.07973150163888931, 0.07930605858564377, 0.07910392433404922, 0.08199161291122437, 0.08554606139659882, 0.07495845854282379, 0.08452534675598145, 0.08374930173158646, 0.08217233419418335, 0.08915631473064423, 0.07476989924907684, 0.08149880170822144, 0.07041479647159576, 0.08738546073436737, 0.08351977914571762, 0.07942347228527069, 0.08255182206630707, 0.0776275098323822, 0.0872935950756073, 0.08370282500982285, 0.08755524456501007, 0.07708212733268738, 0.08166676759719849, 0.0821341723203659, 0.08891549706459045, 0.08699384331703186, 0.08355392515659332, 0.08251550048589706, 0.07556761056184769, 0.0868247002363205, 0.0760570615530014, 0.07881851494312286, 0.08100948482751846, 0.08342038840055466, 0.07931052893400192, 0.07849383354187012, 0.08236616849899292, 0.08089455962181091, 0.08793140947818756, 0.08099807798862457, 0.08462078869342804, 0.08377035707235336, 0.08395744115114212, 0.07689762115478516, 0.08168069273233414, 0.08273144066333771, 0.08688347786664963, 0.08654263615608215, 0.07829852402210236, 0.08356808871030807, 0.08484288305044174, 0.08047640323638916, 0.0771397203207016, 0.07662069797515869, 0.08629447221755981, 0.07291018962860107, 0.07728761434555054, 0.0804019570350647, 0.07850472629070282, 0.07792289555072784, 0.07681220769882202, 0.07871612906455994, 0.07989321649074554, 0.07786950469017029, 0.08704222738742828, 0.07745485752820969, 0.08388803154230118, 0.07870317995548248, 0.07688778638839722, 0.08437785506248474, 0.08115407824516296, 0.07947058975696564, 0.08009016513824463, 0.0859999880194664, 0.08705534040927887, 0.08316922187805176, 0.0738089382648468, 0.08138255774974823, 0.07695017755031586, 0.08340208977460861, 0.07819309085607529, 0.0796007588505745, 0.07835952937602997, 0.07695981860160828, 0.09047862887382507, 0.08125898241996765, 0.08158471435308456, 0.08933231234550476, 0.08656512200832367, 0.08799728751182556, 0.07895395904779434, 0.08083927631378174, 0.08704297244548798, 0.07979167997837067, 0.07824475318193436, 0.0766344964504242, 0.08792141824960709, 0.07496072351932526, 0.08490236848592758, 0.08864195644855499, 0.07986541092395782, 0.0784725546836853, 0.08403883129358292, 0.08238592743873596, 0.0803978368639946, 0.08528617024421692, 0.08185240626335144, 0.07456515729427338, 0.07410605251789093, 0.08514977991580963, 0.08405382186174393, 0.07851874828338623, 0.0898219496011734, 0.08704628050327301, 0.07767899334430695, 0.07781437039375305, 0.0856754332780838, 0.08328495919704437, 0.08779554069042206, 0.07914312183856964, 0.09213674813508987, 0.08913269639015198, 0.07920604944229126, 0.08175201714038849, 0.08582720160484314, 0.07640135288238525, 0.08292056620121002, 0.07920372486114502, 0.08058181405067444, 0.08500108867883682, 0.0845218151807785, 0.08429089188575745, 0.08021129667758942, 0.08164139091968536, 0.08368368446826935, 0.0831180140376091, 0.08563746511936188, 0.08176881074905396, 0.08768334984779358, 0.08129623532295227, 0.08543156087398529, 0.08249026536941528, 0.07988116890192032, 0.07212142646312714, 0.08220097422599792, 0.08087601512670517, 0.07735893875360489, 0.07535191625356674, 0.08287228643894196, 0.08024601638317108, 0.08243270218372345, 0.0767265111207962, 0.08254632353782654, 0.08044268190860748]\n",
            "[0.08483822643756866, 0.07727351784706116, 0.08414161205291748, 0.08524221926927567, 0.08313155174255371]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-3 Designing a score based on Discriminator's score and Generator's reconstruction error**"
      ],
      "metadata": {
        "id": "IQyBlXwz2IID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    min_val = min(data)\n",
        "    max_val = max(data)\n",
        "    normalized_data = [(val - min_val) / (max_val - min_val) for val in data]\n",
        "    return normalized_data\n",
        "norm_discriminator_score = normalize(discriminator_anomaly_score)\n",
        "norm_generator_recon_error = normalize(generator_recon_error)\n",
        "\n",
        "anomaly_score = [disc_score + gen_score for disc_score, gen_score in zip(norm_discriminator_score, norm_generator_recon_error)]\n",
        "#anomaly_score = [0.7 * disc_score + 0.3 * gen_score for disc_score, gen_score in zip(norm_discriminator_score, norm_generator_recon_error)]\n",
        "anomaly_score = [score[0] for score in anomaly_score]\n",
        "print(\"Output By Vamsidhar Vuddagiri\")\n",
        "print(anomaly_score)\n",
        "print(anomaly_score[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Ltl0m22Gj0",
        "outputId": "b20cc3e3-7dca-4fc4-8789-6c139a1de572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output By Vamsidhar Vuddagiri\n",
            "[1.2875819, 1.2361817, 0.5658106, 1.2359478, 0.53041905, 0.635278, 1.0055411, 0.47631326, 1.3004619, 1.5245037, 0.9738647, 0.75919706, 0.5022348, 0.60265535, 1.1314658, 0.29379654, 0.56588113, 0.7993789, 0.38711825, 0.3110165, 0.8939178, 0.33971214, 0.36155513, 1.2182148, 0.6485368, 1.165553, 0.54839677, 1.4201628, 0.34968507, 0.5206655, 0.41902885, 0.21422791, 0.5828776, 0.53394234, 0.6677985, 0.48965383, 1.1848066, 1.0795095, 0.39975938, 1.2168689, 0.5290753, 0.38717237, 0.61874133, 1.2920365, 0.39679575, 0.6294352, 0.5619885, 1.1888382, 1.4257975, 1.4769931, 0.53764755, 0.66381574, 0.48718885, 0.59613687, 1.2069747, 0.6850238, 0.5397024, 1.2619779, 0.35905808, 1.4851136, 1.4242991, 0.46166125, 0.5126261, 0.5257304, 0.7408407, 0.60358095, 0.22783951, 0.55264944, 0.25701678, 0.49932382, 0.50777614, 0.33808622, 0.84235877, 0.31577897, 0.37372237, 0.36539456, 1.143275, 0.48202533, 0.79599005, 0.5451123, 0.7457806, 0.5847534, 0.5179899, 0.6823398, 1.5460596, 0.2807096, 0.71363294, 0.47855294, 1.0470831, 0.165591, 0.6404204, 0.40006116, 0.57244366, 0.591687, 0.4565721, 0.5297586, 1.3234391, 0.54493195, 1.3144336, 0.47640857, 0.2571958, 1.3863661, 0.8039762, 0.6484616, 0.48598537, 0.253382, 1.1424787, 1.511206, 0.45501983, 1.248915, 0.345043, 0.3782375, 0.761981, 1.1649657, 0.50061077, 0.486691, 1.6277052, 0.5068598, 0.8145055, 1.1429071, 0.50356454, 0.41774964, 0.45470548, 0.61814076, 0.70269245, 0.3587085, 1.1864809, 0.43484157, 0.7940331, 0.69923323, 1.4177512, 0.24835418, 0.85078394, 0.4700197, 0.570504, 0.2810772, 1.0619113, 0.90832984, 1.4444118, 0.34160906, 0.36946267, 0.43224952, 1.5332931, 0.17782147, 0.555617, 0.47323668, 0.6652884, 0.68489796, 1.2829834, 0.27809048, 0.40934476, 0.6093543, 0.35847488, 1.0320997, 0.46574917, 0.9310738, 0.38331458, 0.7773649, 1.1415217, 0.2712059, 0.38059276, 0.266222, 1.3866544, 0.52604413, 0.47391987, 0.07284968, 0.4623008, 0.6128868, 0.6283502, 1.4188879, 1.1869649, 0.40664512, 1.0926423, 0.6806844, 0.5052705, 1.1538274, 0.59077936, 0.290484, 0.8648846, 1.4950571, 0.43910185, 0.55121785, 0.3546687, 0.3281241, 0.42772195, 0.19070807, 0.41965276, 0.4962075, 0.41635975, 1.1214714, 1.6232758, 1.0812712, 1.1339903, 0.4014167, 0.23021875, 0.7240188, 0.6343034, 0.32375526, 0.6159114, 0.24451815, 0.4894473, 0.51589644, 0.6326457, 0.2393063, 0.33064243, 0.55604434, 1.1834995, 0.6041992, 0.35255614, 0.42897344, 0.9964646, 0.9098563, 0.47263992, 0.47480452, 1.2065933, 0.41781342, 0.6841279, 1.3236214, 0.73560303, 0.64182645, 0.32871717, 1.0699669, 0.27030736, 0.49329716, 0.7809983, 0.30776978, 0.41597232, 1.5478723, 0.42688185, 0.9219673, 0.77164423, 0.28431225, 0.58699286, 0.5979572, 0.6102854, 0.59393287, 0.377654, 1.0300996, 0.3040271, 0.5012204, 0.44856688, 0.34801614, 0.45385572, 0.6213057, 0.49468026, 0.5168274, 0.74738526, 0.34704924, 1.4858487, 1.3338945, 0.52224666, 0.48936278, 0.5009165, 0.31534725, 0.4279639, 0.6003538, 1.6900625, 1.4206827, 0.37360233, 0.33555186, 0.35210443, 0.495057, 0.71933985, 0.4973911, 0.4806529, 0.41817412, 0.5992907, 0.72176707, 1.293184, 1.2345345, 0.44908172, 0.9226651, 0.4176823, 0.53176975, 1.3025591, 1.424447, 0.46853757, 0.57138973, 0.6071063, 0.6472234, 0.4379169, 0.57787657, 0.11536376, 0.56618154, 0.3294171, 0.5442715, 1.4914641, 0.71143484, 0.34621698, 0.46904558, 0.40959877, 0.46509418, 0.33429274, 0.41910088, 0.43302068, 0.60064924, 0.57915246, 0.5424952, 0.6748998, 0.653782, 0.5382378, 1.216442, 0.40737665, 0.6660604, 0.4217698, 0.95972574, 0.5372675, 1.1785104, 0.62187225, 0.5268398, 0.19344184, 0.36266464, 0.46798307, 1.3445582, 1.2144217, 0.36900008, 1.2269711, 1.2914784, 0.64153826, 0.6691372, 0.39105532, 1.1176192, 0.6078843, 0.6811935, 0.60430723, 0.36742866, 0.5056207, 1.5375535, 0.40644908, 0.78618664, 0.47040495, 0.70238125, 1.3017584, 0.63469416, 0.81979555, 1.4403753, 0.4837982, 1.58322, 0.66199243, 0.48695964, 0.461951, 0.5558853, 0.45087206, 0.97642356, 0.5779904, 0.55647075, 0.40354398, 0.43420747, 0.7763068, 0.419026, 1.3892463, 0.47423655, 0.47588205, 0.60732484, 0.48968616, 0.6904131, 0.4470637, 0.5707984, 0.54046416, 0.6246314, 0.37977043, 0.60577387, 0.57297945, 0.26063815, 0.46429378, 0.5564644, 0.5948442, 0.3588615, 0.49963573, 1.4051751, 0.34343433, 0.489659, 1.2353203, 0.41665605, 0.35326692, 1.3688244, 1.3297629, 0.42893717, 1.1303447, 1.3169003, 0.2943318, 1.0384634, 1.3526485, 0.5703345, 0.68813485, 0.33038357, 0.32983014, 0.39904606, 0.7035091, 1.2426295, 0.36827856, 1.1336385, 0.33448932, 0.34742802, 1.3890524, 0.61231714, 0.2567841, 0.48938566, 0.88114035, 0.528028, 0.24969706, 0.64671826, 0.42392662, 0.51773536, 0.67761916, 0.63064575, 0.48679465, 1.4820604, 0.49014094, 0.33808282, 1.2885083, 0.4651704, 0.7131955, 0.52894664, 0.6863458, 1.6190522, 0.758102, 0.58879113, 1.2237556, 0.3955628, 0.67269707, 0.3760148, 0.48834738, 0.37543, 1.268883, 0.617565, 1.3449595, 0.45630422, 0.37261423, 0.42825267, 0.5165627, 0.72355074, 0.4269489, 0.47295892, 0.27445602, 0.9847387, 0.30215633, 1.262135, 0.6955405, 0.5492679, 0.46270704, 0.55845207, 0.29634875, 0.63702404, 0.5330058, 0.5109293, 0.6964361, 0.48218665, 0.48797646, 1.3643013, 1.0632367, 0.45728794, 0.6034251, 0.35744357, 0.4848179, 0.3829948, 0.6813287, 0.44896322, 0.7184268, 0.5363984, 0.5694905, 0.67841744, 0.74328357, 1.1288855, 0.7348949, 0.6125265, 0.6424483, 0.5453128, 0.56911874, 1.2986598, 0.30188012, 0.4433308, 0.31038144, 1.1933514, 0.54970366, 0.5297533, 0.44992545, 0.7847503, 0.35555384, 0.5758965, 0.4788568, 0.5649042, 0.6236812, 0.13192368, 0.6835609, 0.8508171, 0.22927636, 0.6013202, 0.22140007, 0.8331717, 0.20199645, 0.41716486, 1.1155946, 0.4190257, 1.4971521, 0.6194493, 0.4870458, 0.5591243, 1.357872, 0.7370664, 0.64204156, 0.42482963, 1.6046737, 1.3040069, 0.35151505, 1.499612, 0.52178836, 0.31999362, 0.6794768, 1.0382197, 1.2634825, 0.64712745, 1.1189343, 0.45529264, 0.40453836, 0.30191332, 1.3052001, 1.2937089, 0.42240593, 1.0315022, 0.5114023, 0.5601166, 0.46685192, 0.30842513, 0.41011098, 1.4685316, 1.1717753, 0.43215552, 0.43540132, 0.5046951, 0.5576525, 1.097018, 0.38334814, 0.8566425, 0.47245845, 0.54238087, 0.5306221, 0.39027083, 1.5530632, 1.1775012, 0.32540306, 0.6001872, 0.22644955, 1.0778342, 0.6234167, 0.47373635, 0.19306996, 0.5551927, 1.179483, 0.72157294, 0.5530601, 1.2885807, 0.49374044, 0.63838804, 0.23076302, 1.184633, 0.48008132, 1.1954318, 0.73861027, 1.1967568, 1.3423685, 0.2410434, 0.3983026, 0.30663615, 1.268695, 0.7895752, 0.74212897, 0.25176886, 0.44073844, 0.55483407, 1.1185559, 1.0533046, 0.9417945, 0.6034265, 0.7097027, 0.3024106, 0.46994248, 0.28924632, 0.5812615, 1.2379112, 0.3500658, 0.5045716, 0.4813611, 1.203437, 0.7846895, 0.66363233, 0.3910916, 0.35201722, 0.38563672, 0.2674148, 1.1193432, 0.57134587, 0.6688589, 0.6124812, 0.654583, 0.30818582, 0.27765048, 0.3372414, 0.44268662, 0.5317186, 0.38936174, 0.24961066, 0.7111842, 0.64549595, 0.6217512, 0.45193276, 0.25092876, 0.44210404, 0.29547912, 0.61255354, 0.25618133, 1.5589305, 0.39783478, 0.49093404, 0.3932151, 0.48537865, 1.7237345, 0.46757373, 0.64307123, 0.4123412, 1.1519268, 0.4297689, 1.1353934, 1.5189276, 1.0567807, 1.1948115, 0.2227075, 0.64149594, 0.3662289, 0.4198786, 0.38806486, 0.7073038, 0.4849826, 0.40334556, 0.6698773, 0.578305, 0.43739423, 0.49658707, 1.0662427, 1.4159775, 0.47010642, 0.55712694, 0.46346176, 1.5224092, 1.127681, 0.3760096, 1.3346779, 0.6141477, 1.1867005, 0.46111155, 0.66295314, 0.47608185, 0.60925984, 0.6349036, 0.53509986, 1.4697082, 0.23645495, 0.96028584, 1.1446239, 0.63149625, 0.3456568, 0.27537924, 0.93449175, 0.456983, 0.6727881, 0.59505635, 0.5124942, 0.968609, 0.771446, 0.5234463, 0.20932104, 0.78684974, 0.300679, 0.55340785, 0.3367804, 0.45931795, 0.6022229, 1.3004005, 1.3843422, 0.48177093, 0.29335204, 1.0443871, 1.528393, 1.4856386, 0.21756615, 1.4940418, 0.5731146, 0.44290316, 0.47761363, 0.4120277, 0.60676014, 0.5381812, 0.75848514, 0.2269211, 0.46898502, 0.48678803, 0.5821858, 0.6441228, 1.149799, 0.70507383, 0.32131407, 1.2481481, 0.38180253, 1.3241787, 0.782333, 0.42321235, 1.5414855, 1.4619999, 0.56062984, 0.40780553, 0.5048289, 1.3548415, 0.5748927, 1.5820335, 0.29581514, 0.6669813, 0.7925007, 0.6563138, 0.97699726, 0.5163747, 0.55057245, 1.1890687, 0.5221011, 0.20917925, 0.20555888, 1.4047418, 0.5732676, 0.37608084, 0.5717571, 0.40676376, 0.5587487, 1.488075, 0.2422657, 0.5988508, 0.7349127, 0.16342175, 0.29262367, 0.5638422, 0.2874875, 0.5777885, 0.67835057, 0.613772, 0.45176494, 0.5008951, 0.784182, 0.54377073, 0.14302504, 0.508793, 1.3706175, 0.4553229, 0.6124315, 0.5611455, 0.26620868, 0.3760247, 0.53417075, 0.5746923, 0.3987842, 0.8378571, 0.15079133, 0.41412663, 0.601632, 1.1499276, 0.42288128, 0.32102305, 0.49614638, 0.2857883, 0.94562805, 0.3919411, 0.7645656, 0.22604206, 1.1762797, 0.38717884, 0.41482773, 1.0079606, 0.598473, 1.3903632, 0.28481558, 0.5810228, 0.44550464, 0.54290766, 0.57612604, 0.7194836, 0.52254623, 1.2247597, 0.39193848, 0.35436708, 0.26167667, 0.263872, 0.5211519, 0.55169505, 0.5012258, 1.3447021, 0.8175248, 0.5096101, 0.38017324, 0.3972396, 0.20864359, 0.5446508, 0.5103997, 0.21851328, 0.5385649, 0.3958709, 0.65587133, 0.33978504, 0.47128862, 1.2426226, 0.4734531, 0.99830556, 0.4168452, 1.108011, 0.4495443, 0.6043015, 0.5803577, 1.2656629, 0.54317707, 0.6638972, 0.41179642, 0.46730813, 1.2965037, 0.7078228, 1.2073704, 0.36816937, 1.5812197, 0.41928834, 1.0885353, 0.47093982, 0.4237368, 0.4422741, 0.35719994, 0.36526805, 0.63818777, 0.51987714, 0.802856, 0.38066062, 0.6447172, 0.46574518, 1.0816716, 1.2319694, 0.49782345, 0.599709, 0.34390005, 0.5061962, 0.9890185, 0.34621567, 0.41734526, 0.9553503, 0.48676592, 0.5534198, 1.7030826, 1.1407274, 0.43248916, 1.3404276, 1.3064786, 1.3867295, 0.24778296, 1.6957319, 1.5199856, 1.21167, 0.6090889, 1.0286808, 0.32495558, 0.47004312, 1.4357011, 1.5071799, 1.0179956, 0.3883724, 0.96874166, 1.5809491, 1.3541518, 0.589031, 0.3846554, 0.44280943, 0.426562, 0.6362998, 1.0784583, 0.56728274, 0.37617195, 0.6149888, 0.41845083, 0.6523088, 1.1864543, 0.5689112, 1.0579174, 0.5091142, 0.45461854, 0.30277038, 0.41544142, 0.78897667, 0.42540944, 0.5865906, 0.41957605, 0.5751451, 0.5617156, 0.5094976, 1.2983809, 0.97225255, 0.62811726, 0.5324315, 1.7501681, 1.484936, 0.3687933, 0.38260555, 0.60264015, 0.7860201, 0.44473374, 0.99503684, 0.5754406, 0.6483042, 0.47341314, 1.269066, 0.5840277, 0.7065474, 0.43025813, 0.35074195, 1.321758, 0.25202948, 0.31546423, 0.65215874, 0.75386906, 0.47274694, 0.42414525, 0.5258518, 1.2912486, 0.4561353, 0.78140926, 0.81631, 0.50697935, 0.47622007, 0.32566217, 0.42521876, 0.37684184, 0.6679918, 1.0061142, 0.25376597, 0.31852072, 0.46404105, 0.75900334, 0.8103766, 0.3962189, 0.5378807, 0.24587455, 0.4574502, 0.5909697, 0.48805714, 0.5566053, 1.5054684, 0.9930764, 0.38597274, 1.509695, 0.25968033, 1.2611283, 0.30718726, 0.44671962, 0.1865631, 0.6028124, 0.7152429, 0.4461928, 1.2154146, 0.6110909, 0.27707484, 0.39737645, 1.6382538, 0.26071963, 0.50108254, 0.1439296, 1.1678452, 0.5492982, 0.5760962, 0.43142444, 0.51632065, 0.5566797, 1.368182, 0.4887855, 0.49892017, 0.5001563, 0.36914235, 0.94606847, 0.5368375, 1.2483486, 0.5598903, 0.5235774, 1.5434864, 1.6071435, 0.9883659, 0.18500952, 0.7315424, 0.5746244, 0.57288325, 0.5519877, 1.0481147, 0.48173004, 0.9910065, 0.40517128, 0.25965813, 0.323407, 0.66894454, 1.4854858, 0.4450901, 0.7152962, 1.3890197, 1.3258598, 1.2652037, 0.55741763, 0.8665799, 0.55693537, 0.6336443, 0.4846226, 0.41920948, 0.5338668, 1.3324218, 0.6424131, 0.42411017, 0.44935697, 0.68801737, 0.2539905, 0.5242692, 0.47768465, 0.55011666, 0.56910104, 0.5988545, 0.47331917, 0.6312863, 0.34098405, 1.0814815, 1.5821135, 0.46824884, 0.5264248, 0.4590822, 0.5497039, 0.49170792, 0.30710658, 1.1606023, 0.68690705, 0.33763626, 0.5987947, 0.2026115, 0.23673171, 1.6013345, 0.56452054, 1.073985, 0.48139215, 0.45026642, 0.28516018, 1.2207662, 0.84187967, 0.53425246, 0.7204915, 0.5158549, 1.2103223, 0.40774316, 1.0622635, 0.45039982, 0.30441242, 1.3140037, 0.28872418, 0.282949, 0.3973054, 0.26119474, 0.46152154, 0.30997258, 0.5639999, 1.3989699, 0.39230293, 0.42117476, 0.60579216, 0.30461746, 0.6840179, 0.55107737, 0.52513415, 0.83383197, 0.4890745, 0.5364887, 0.5647692, 0.23650202, 1.2252345, 0.40843287, 0.54746485, 0.32700795, 0.30146948, 1.1020972, 0.5041011, 0.763419, 1.0017343, 0.6264063, 0.63721395, 1.0423331, 1.3000162, 0.29812488, 0.34575933, 0.54788744, 1.1146384, 0.53198624, 0.5846901, 0.38865542, 1.3875775, 1.2999624, 0.43154413, 0.52128816, 0.7218958, 0.6333733, 0.44993904, 1.2963827, 0.3056269, 0.5700936, 0.5797126, 0.79501283, 1.2719531, 0.31209183, 0.70402205, 0.61068654, 1.0002384, 0.48935148, 0.30996728, 0.6037094, 0.36515772, 0.27438134, 0.32422283, 0.49462074, 0.4975358, 0.3926799, 0.5344804, 0.64803135, 0.565731, 0.6734941, 1.0369732, 0.7072163, 0.53320587, 0.38183454, 1.030352, 0.5160239, 0.33615246, 0.35256156, 0.5518354, 0.8487321, 1.1941197, 0.43794352, 1.3576496, 0.26738948, 0.4209117, 1.196922, 0.68550545, 0.27822953, 0.4073453, 1.2236515, 0.42254952, 1.6050134, 0.47364184, 0.42446622, 0.43953887, 0.31195486, 0.19185533, 0.46091878, 0.6509401, 0.25754982, 0.72368747, 0.72497714, 1.0056628, 0.527998, 0.78215533, 1.330632, 1.0275524, 0.46693674, 0.69280237, 0.32612282, 0.4212641, 0.6695684, 0.48040342, 0.39601368, 0.50511837, 0.65949875, 0.47371468, 0.5577396, 0.8970252, 0.48255813, 1.0686884, 0.74266744, 0.51169336, 1.0677884, 1.0598809, 0.55502695, 0.1710664, 0.5190697, 0.4363809, 1.6655242, 1.0000129, 0.39868772, 0.48563656, 0.54108423, 0.5798346, 0.8698065, 0.48704922, 1.4775258, 1.1894413, 1.230746, 0.4791993, 0.37694815, 1.3839071, 0.4767417, 0.715958, 0.53500324, 1.543334, 0.40448928, 1.7058101, 0.29940787, 0.40462506, 1.2573409, 0.4494989, 1.2493212, 0.27856317, 0.3738271, 0.43260255, 0.4441701, 0.45756856, 0.3568586, 0.5822251, 0.41982302, 1.2231061, 0.62145346, 0.96735495, 1.3891207, 0.5985887, 0.568227, 1.2367688, 1.4655423, 0.5056247, 0.05387018, 0.44650546, 0.5007775, 1.2638059, 0.48776162, 0.4395794, 0.4298352, 1.3829055, 0.7368244, 0.7902343, 0.52625877, 0.66885495, 0.841313, 1.4201143, 0.68208945, 0.58935153, 0.3635873, 0.25905266, 0.6659906, 0.42753735, 0.55397797, 0.4163065, 0.46133122, 1.1116558, 1.1979351, 0.73557884, 0.7754142, 0.5380895, 1.5179927, 0.45573434, 1.2688035, 0.42132407, 0.64123124, 0.54410166, 1.1212027, 0.27057916, 0.5354833, 0.71497744, 0.5452366, 1.0759242, 0.32344902, 0.21067025, 0.39484176, 1.1961187, 0.38100863, 0.4197671, 0.5956923, 0.48293555, 0.33604395, 0.47714686, 0.4681653, 0.6685996, 0.7137536, 0.7570616, 0.38114473, 0.8213214, 0.5677391, 0.57597214, 0.45508894, 0.30795556, 0.723446, 0.36801642, 1.1014006, 0.37901726, 0.3630907, 0.46586657, 0.48539588, 0.45477214, 0.28502908, 0.5137375, 0.59025407, 0.30221543, 0.3857531, 0.4982179, 0.52504224, 0.4451547, 0.42241794, 1.2062227, 1.3626387, 0.3205004, 0.62619424, 0.5367153, 0.2772281, 1.0875884, 0.4158029, 0.6290373, 0.64137155, 0.9070973, 0.47164577, 0.934994, 0.33180246, 1.253222, 0.57881457, 0.5576906, 0.38328952, 0.47007757, 0.5646998, 1.2567672, 0.49844164, 1.5688434, 0.40972146, 0.65300983, 1.3683066, 0.7124102, 0.6632382, 0.5570324, 1.4159184, 1.3473634, 0.5212422, 0.10628644, 0.10794589, 0.63644207, 0.5162974, 0.22919333, 0.9300512, 0.5052209, 0.54463583, 1.3278298, 1.2531196, 0.6164832, 1.3401251, 0.5705802, 1.126385, 0.55442315, 0.4106639, 0.40021533, 0.5283556, 0.34579378, 0.52001154, 1.5283399, 0.3560877, 1.7348874, 0.4952307, 0.2815708, 0.5012339, 1.1351728, 0.5935016, 0.51068723, 0.31966436, 0.5047914, 0.5884206, 0.42965144, 0.3809473, 0.37605813, 0.47058538, 0.916488, 0.50074834, 0.60109746, 0.41316018, 0.6307376, 0.443991, 0.2784523, 0.48754284, 0.52976805, 0.5069655, 0.63824314, 1.2027612, 0.656778, 1.1594238, 0.5869408, 0.29165235, 1.1356294, 0.507747, 0.39998165, 1.2720238, 0.71516436, 0.5255406, 1.1746595, 1.4287249, 0.6876616, 0.31914026, 0.45308116, 0.26534775, 0.61218405, 0.22662657, 0.45339572, 0.40408126, 0.46386084, 0.49406052, 0.48340294, 0.6664798, 0.3433607, 0.40881217, 0.4788952, 0.533588, 1.3580439, 0.43598634, 0.45541033, 0.29918075, 0.26682428, 0.32253164, 0.28380892, 1.3881032, 0.51501435, 0.67867213, 0.061058514, 0.31166407, 0.4696379, 0.57929397, 0.63272583, 0.49412748, 0.42006168, 0.992442, 0.37485588, 1.1688746, 1.3823309, 0.62958455, 0.4901634, 0.28308395, 0.47578463, 0.4271899, 1.3748894, 0.49682698, 0.59567106, 0.72650903, 0.697183, 1.2419419, 1.0117451, 1.3838823, 0.6298699, 0.45388368, 0.7183119, 1.420191, 0.63142526, 0.4488201, 0.43891844, 1.0954815, 0.4377605, 0.60884607, 0.4483403, 1.5111728, 1.4320801, 0.3992406, 0.5947854, 1.4862325, 0.11857126, 0.37936056, 0.8513825, 0.45561397, 0.32097176, 1.0039929, 1.0639677, 0.45820898, 0.8192454, 0.7469479, 0.551123, 0.9604181, 0.45745903, 0.4518675, 0.42771628, 0.62679034, 0.25332785, 0.31585386, 0.6069497, 1.2061677, 1.1701437, 0.475556, 0.28076524, 0.5223815, 0.9288116, 0.9343549, 0.37102827, 0.4545063, 1.1663889, 0.48342487, 0.31050554, 0.64646345, 0.613687, 0.69926727, 0.72984046, 0.14560452, 1.0172546, 0.66127735, 0.3081054, 0.6682463, 1.401986, 0.5188128, 0.46141475, 0.99270654, 0.9516269, 0.2370076, 0.39256874, 0.32591417, 0.59637094, 0.50967115, 0.4940998, 0.4956415, 0.5444889, 0.5448808, 0.86692363, 1.3018396, 0.5727966, 0.4748187, 1.4111034, 1.5103319, 0.30399916, 0.36083144, 0.50385314, 1.491817, 0.57320964, 0.76114625, 0.6000167, 0.7815974, 0.3047312, 1.6446519, 0.6242455, 0.63885415, 0.48213938, 1.0917745, 0.6310906, 0.80063486, 0.24316792, 0.30029106, 0.41186506, 0.5004353, 0.60193217, 0.34167904, 0.6614322, 0.6434533, 0.40715367, 1.088871, 0.527686, 1.261246, 0.4480625, 0.5552611, 0.58980006, 0.48760498, 0.5494118, 0.5480767, 0.8951576, 0.40323827, 0.38203537, 1.1599753, 0.36043882, 0.5706878, 0.5771318, 0.15603815, 0.3971652, 0.672255, 0.3689958, 0.940817, 0.5973802, 0.32398343, 1.3804085, 0.55216885, 1.1420352, 0.4994487, 0.5333226, 1.3271806, 0.71888626, 0.6088377, 1.67718, 0.40347844, 0.43498492, 0.6554897, 1.4094918, 0.43100163, 1.258637, 0.34878582, 0.77478975, 0.46392936, 0.49488753, 0.5351591, 0.74744827, 0.2563223, 0.17059645, 1.2458428, 0.24004641, 1.4477339, 0.3573771, 0.5499016, 0.3251449, 0.18010722, 0.47967106, 1.0876453, 1.2604262, 0.81332606, 0.28930274, 0.5836591, 0.53878605, 0.8521641, 0.30051062, 0.25880107, 1.4159532, 0.40632114, 1.2974675, 0.53567004, 1.3894439, 1.3558846, 0.60275, 1.4679446, 0.11330143, 0.4874828, 0.63216823, 0.5464097, 0.28490093, 0.34846053, 0.9855335, 0.31939486, 0.34760416, 0.36973888, 0.5599558, 0.6854209, 0.4600763, 0.35237148, 0.46603242, 0.42971173, 0.5517152, 0.75006884, 0.6426188, 0.38408157, 0.68629205, 1.519521, 1.159667, 0.4404109, 0.64674044, 0.50950855, 0.40661213, 0.66829085, 0.38725373, 0.30268523, 0.6925502, 0.31847247, 0.31579053, 0.71328306, 0.5418869, 1.2582095, 0.3730225, 1.052156, 0.5248608, 1.4265829, 1.4893423, 0.4206913, 0.33237237, 0.45250422, 0.9886189, 0.35570264, 0.64006406, 1.3415744, 1.5842988, 0.42575747, 0.5175563, 0.732673, 0.4062942, 0.5145847, 0.544798, 0.73800963, 0.34909007, 1.0458701, 1.0435402, 0.48838502, 1.3889685, 0.33433366, 0.2532266, 0.5229759, 0.5128595, 0.3498742, 0.3312757, 0.51396906, 0.5669919, 0.46003687, 0.57709366, 0.93389434, 1.0400548, 0.64997905, 0.67784005, 1.5468495, 0.68905383, 0.30771068, 0.58851874, 1.6024814, 1.4797182, 0.35717034, 0.3951511, 0.96640503, 0.6667762, 0.91729325, 0.59330034, 0.2890197, 0.5238648, 0.5935544, 0.40925395, 0.50637317, 0.7714309, 0.4438349, 0.56190205, 0.2675015, 0.49128282, 1.4047613, 0.54902583, 0.49072498, 1.2934217, 0.4363187, 0.38197038, 1.1965094, 0.4813597, 1.2798712, 1.4033356, 0.5688504, 1.0041797, 0.837403, 0.66941756, 0.52802277, 1.166598, 0.410954, 0.49730262, 0.9482059, 0.55962825, 0.31711966, 0.4396761, 0.36051923, 0.369103, 0.26898587, 0.57181764, 0.6115256, 0.4147332, 1.2833971, 0.74293244, 1.0026519, 0.3783062, 0.5869913, 0.6242612, 0.48109993, 0.23317322, 0.84339124, 0.5303674, 0.5110112, 0.3580856, 0.7346918, 0.38286847, 0.47219175, 0.9571177, 0.5933558, 0.5539216, 0.74765587, 0.36854866, 0.66717094, 1.3284583, 0.16550374, 1.3611195, 1.1500475, 1.5953145, 0.4137345, 0.6417894, 0.46178025, 0.4073171, 0.39580035, 0.50088644, 1.1561435, 1.6509624, 0.579037, 1.4491029, 0.5480183, 1.3523064, 0.42232108, 1.2639483, 0.6563156, 0.5843128, 0.5590476, 0.71779376, 0.5955408, 0.14851332, 0.34914932, 0.62486166, 1.2806222, 0.91971177, 1.5291831, 0.5594689, 0.61388403, 0.6635413, 0.6755629, 0.21717909, 0.51448333, 0.45137852, 0.7145535, 0.9321289, 0.07533805, 0.4690478, 0.43347415, 0.98410153, 0.9845028, 1.1829879, 0.6494531, 0.44525254, 0.5263063, 0.4967215, 0.33672234, 0.47073022, 0.40760687, 0.4582308, 0.54468817, 0.06295617, 0.70725584, 0.38524565, 0.2966571, 1.2989473, 0.5054947, 1.1376159, 0.7019803, 0.5743379, 0.45315087, 0.4616569, 0.74941903, 0.5445508, 1.0975049, 0.5799458, 0.3474719, 0.6878828, 1.5307746, 0.68526083, 0.9906173, 1.2345926, 1.3627801, 0.41232193, 1.1037157, 0.35465527, 0.47451818, 1.18133, 0.4445711, 1.4093895, 0.69616663, 0.57049924, 0.40975827, 0.37117392, 0.5735758, 0.54790807, 1.4460015, 0.7436191, 0.4556521, 1.1367595, 0.700622, 0.2880412, 0.40851277, 0.15742573, 0.7001659, 0.1677055, 1.173333, 0.4555041, 0.53639764, 0.41074798, 0.42594126, 0.5462576, 0.47439373, 0.6361914, 0.6063232, 0.26108524, 1.1575243, 0.62735796, 0.40714532, 0.344966, 1.3839538, 0.3588206, 1.5995947, 0.18409598, 1.2729301, 1.1656044, 0.6284738, 0.6046259, 0.7163154, 0.75255257, 0.6480241, 1.0268551, 0.51510465, 0.43348014, 1.2661418, 0.52711815, 0.60311794, 0.48143244, 0.9165536, 0.5703588, 1.6867175, 0.39672244, 0.45324105, 0.48371693, 0.29056495, 0.2606507, 0.5284689, 0.227245, 0.5880695, 1.1028335, 0.60524863, 1.2674999, 0.24432103, 0.62779105, 1.0157238, 0.94981277, 0.38665557, 0.4004825, 0.21521842, 0.79801977, 0.4540035, 0.43886793, 1.1703162, 1.3008065, 0.502899, 0.5744476, 0.46056372, 0.5214505, 0.49700117, 0.45704356, 0.3406146, 0.9050288, 1.0883585, 0.5729945, 0.46773088, 1.4465683, 0.4845926, 0.5631347, 1.1452341, 0.7270061, 0.4821963, 0.38296425, 0.5469143, 0.5476841, 0.58434606, 0.255755, 0.8649272, 1.2734387, 0.46684003, 0.34312463, 0.2808036, 0.3257495, 0.881842, 0.3402997, 0.53407335, 1.0245674, 1.4357803, 0.5967495, 0.6565874, 0.2745543, 0.67862105, 0.50302535, 1.1567876, 0.6421281, 0.37640014, 0.55964226, 1.4661648, 0.3435069, 0.575579, 0.6067168, 0.1666482, 0.44159237, 1.1859648, 0.21187761, 0.5562427, 0.37372345, 0.46350878, 0.33146936, 0.42552873, 0.49629068, 1.3284633, 0.37794352, 0.55521226, 1.1356485, 1.2708818, 0.96404636, 0.35781986, 0.9132568, 0.4587415, 0.46268597, 0.6017446, 0.1690158, 0.7228328, 0.6104203, 0.3918115, 0.5457407, 1.4927645, 0.5006651, 0.44449484, 0.43799236, 1.3412845, 0.75169015, 0.6211285, 0.7761593, 0.48792765, 1.1556047, 0.5497644, 1.0785611, 1.2101499, 0.43708488, 0.31351748, 1.2150266, 0.58767664, 0.39606014, 0.6780058, 0.6208557, 0.64525837, 0.34822223, 0.6938743, 1.5268426, 0.5351806, 0.59607106, 1.3489957, 0.40320894, 0.32368162, 0.83678246, 1.411017, 0.16288015, 0.29046497, 0.62297684, 0.49330238, 1.0417248, 0.63589126, 0.23351438, 0.61692584, 1.1004432, 0.22533011, 0.57226, 0.8220976, 0.6559197, 0.74218535, 1.2894455, 0.5255824, 0.44457552, 0.66986054, 0.9742784, 0.4363842, 0.5917831, 0.7038957, 0.3801017, 0.4834126, 0.32622695, 0.5828593, 0.8626116, 1.3745619, 0.1559306, 0.64008856, 0.59052575, 0.45423058, 0.21648255, 1.3686159, 0.81664836, 0.34178445, 0.45414212, 0.44428265, 0.6974913, 1.3793075, 0.50727516, 0.5339632, 0.15663132, 1.0869335, 0.39550173, 0.5362579, 0.7656693, 0.8308203, 0.44306633, 0.820886, 0.32582808, 1.3436391, 0.48604813, 0.39002296, 0.27758992, 0.5201134, 0.53565496, 0.6167914, 0.40947163, 0.48891148, 0.40002283, 1.4959548, 0.7890804, 1.3190317, 0.7184689, 0.5950829, 0.50629437, 1.3286605, 0.16732018, 0.6311981, 0.5680748, 0.64114404, 0.50605667, 0.453691, 0.89266944, 0.5660388, 1.0028083, 0.59994125, 0.6242712, 0.5747817, 0.46330434, 0.2515566, 1.5957218, 0.32390743, 0.4813087, 0.8899852, 0.38402006, 0.7232797, 0.60183513, 0.6128673, 0.2833391, 0.29486227, 0.08511834, 0.5793121, 1.3157198, 1.4605182, 0.77685916, 0.55856663, 0.8311347, 1.2203789, 0.54688376, 1.2218568, 0.7902446, 1.459414, 0.82624185, 0.503802, 1.1757497, 0.5450374, 1.1848221, 1.2809527, 0.3779519, 0.52619505, 0.58933955, 0.42169374, 0.26988575, 1.6010288, 1.581835, 0.43241867, 0.2266882, 0.39008835, 0.68505985, 1.5980799, 0.49914807, 1.4905543, 0.4778399, 0.6282278, 0.5565534, 0.6042083, 1.2597389, 1.7805742, 0.16239081, 0.56141067, 0.58178127, 0.4509778, 1.2454767, 1.5198277, 0.718979, 0.90475005, 0.22205585, 0.56260526, 0.41128448, 0.37617993, 0.36962193, 0.46982932, 1.4912465, 0.9659047, 0.5578137, 0.5520644, 1.1270769, 0.7415211, 0.87038934, 1.0836986, 0.08483573, 0.68621206, 1.239093, 1.3603318, 0.4909002, 0.33756253, 1.3822007, 1.5103378, 0.7547028, 0.33943474, 0.4794087, 0.4954718, 0.7125625, 0.65106654, 0.5452186, 1.1358569, 0.26538557, 0.6601379, 0.28253546, 0.3799252, 0.45637813, 0.54053956, 0.41751587, 0.36791772, 0.5188914, 0.45203692, 0.69860166, 0.45566413, 0.5826005, 0.5323534, 1.2067153, 0.3119879, 0.4795823, 1.036596, 1.6076381, 0.62896323, 0.36107424, 0.54571486, 0.5903825, 0.43769944, 0.32047084, 0.30228478, 0.6412448, 0.17227198, 1.0209188, 1.0744057, 0.36829937, 1.2757747, 0.3089951, 1.0309693, 0.4169508, 0.3460418, 1.2959065, 1.1678007, 0.55692536, 0.37525302, 0.33081928, 0.57408834, 1.0704693, 0.40214238, 1.3792005, 0.6309264, 1.6233997, 0.53173894, 0.18256757, 0.4510621, 0.2898851, 0.5180646, 0.35737997, 1.0376937, 0.3632118, 0.31416723, 0.7878538, 1.3636756, 0.4762193, 1.510829, 0.6331839, 0.7009099, 0.3653236, 0.4320095, 0.6506988, 0.39377528, 1.284, 0.30276826, 0.6982515, 0.24412079, 1.2568512, 0.72349846, 0.4159765, 0.36717215, 0.56220925, 1.4206452, 0.43494654, 0.60591483, 0.485599, 0.23026052, 0.19377622, 0.60113585, 0.56304884, 0.3687907, 0.76484436, 1.421159, 0.9941392, 0.34410995, 1.2372444, 0.51445216, 1.6432015, 1.3193355, 1.4081675, 0.7191817, 0.39350402, 0.46246272, 1.3677241, 1.2061002, 0.5137606, 1.0770329, 0.46730062, 1.2225087, 0.5584255, 0.5710412, 0.42809603, 1.3768467, 0.5497652, 0.5048508, 0.61822397, 0.4826699, 0.6899098, 0.44522062, 0.6110092, 0.50794894, 1.3763031, 0.14463446, 0.920006, 1.0151639, 1.0044847, 0.25782782, 0.4978252, 0.42931256, 1.4032288, 0.30599236, 0.5099132, 0.42243674]\n",
            "[1.2875819, 1.2361817, 0.5658106, 1.2359478, 0.53041905]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since, we haven't trained with actual labels, I have assumed a threshold. If the the anomaly score is greater than threshold then the predicted label is 1 else 0\n",
        "#Like this, we will get a list and, we will calculate the Accuracy, Precision, Recall, F1 Score\n",
        "\n",
        "# Determine threshold for anomaly detection\n",
        "anomaly_threshold = 0.4\n",
        "\n",
        "# Classify instances as normal or anomalous based on the anomaly score\n",
        "predicted_labels = [1 if score > anomaly_threshold else 0 for score in anomaly_score]\n",
        "\n",
        "\n",
        "true_labels = [label.item() for _, labels in kdd_data.test_dataloader for label in labels]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(true_labels[:len(anomaly_score)], predicted_labels) * 100\n",
        "recall = recall_score(true_labels[:len(anomaly_score)], predicted_labels)* 100\n",
        "f1 = f1_score(true_labels[:len(anomaly_score)], predicted_labels) * 100\n",
        "accuracy = accuracy_score(true_labels[:len(anomaly_score)], predicted_labels) * 100\n",
        "\n",
        "print(\"Output By Vamsidhar Vuddagiri\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWDt1-YLOr5B",
        "outputId": "77f3d683-5d58-4396-c7d7-bec846100548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output By Vamsidhar Vuddagiri\n",
            "Precision: 80.8930\n",
            "Recall: 83.1821\n",
            "F1-score: 82.0216\n",
            "Accuracy: 70.5095\n"
          ]
        }
      ]
    }
  ]
}